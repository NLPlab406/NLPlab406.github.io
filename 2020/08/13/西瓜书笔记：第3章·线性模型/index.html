<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>西瓜书笔记：第3章·线性模型 | 东北石油大学智能技术与自然语言处理实验室</title><meta name="description" content="西瓜书笔记：第3章·线性模型"><meta name="keywords" content="算法,机器学习,西瓜书"><meta name="author" content="东北石油大学智能技术与自然语言处理实验室"><meta name="copyright" content="东北石油大学智能技术与自然语言处理实验室"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/ava.jpg"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="西瓜书笔记：第3章·线性模型"><meta name="twitter:description" content="西瓜书笔记：第3章·线性模型"><meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200813024012574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:type" content="article"><meta property="og:title" content="西瓜书笔记：第3章·线性模型"><meta property="og:url" content="https://nlplab406.github.io/2020/08/13/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%AC%AC3%E7%AB%A0%C2%B7%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="东北石油大学智能技术与自然语言处理实验室"><meta property="og:description" content="西瓜书笔记：第3章·线性模型"><meta property="og:image" content="https://img-blog.csdnimg.cn/20200813024012574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://nlplab406.github.io/2020/08/13/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%AC%AC3%E7%AB%A0%C2%B7%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><link rel="prev" title="SSH简便远程登录+通过SSH从服务器下载文件" href="https://nlplab406.github.io/2020/08/15/SSH%E7%AE%80%E4%BE%BF%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95+%E9%80%9A%E8%BF%87SSH%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"><link rel="next" title="【自我学习】胶囊网络CapsNet" href="https://nlplab406.github.io/2020/08/11/%E3%80%90%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E3%80%91%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9CCapsNet/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">东北石油大学智能技术与自然语言处理实验室</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/ava.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">21</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">30</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#写在前面"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">写在前面</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#梳理框架"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">梳理框架</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#基本形式"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">基本形式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#回归任务"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">回归任务</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#线性回归"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">线性回归</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#多元线性回归"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">多元线性回归</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#分类任务"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">分类任务</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#对数几率回归"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">对数几率回归</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#线性判别分析（LDA）"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">线性判别分析（LDA）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#多分类"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text">多分类</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#一对一（OvO）"><span class="toc_mobile_items-number">5.3.1.</span> <span class="toc_mobile_items-text">一对一（OvO）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#一对其余（OvR）"><span class="toc_mobile_items-number">5.3.2.</span> <span class="toc_mobile_items-text">一对其余（OvR）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#多对多（MvM）"><span class="toc_mobile_items-number">5.3.3.</span> <span class="toc_mobile_items-text">多对多（MvM）</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#类别不平衡"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text">类别不平衡</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#结尾"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">结尾</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#参考链接"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">参考链接</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#写在前面"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梳理框架"><span class="toc-number">2.</span> <span class="toc-text">梳理框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基本形式"><span class="toc-number">3.</span> <span class="toc-text">基本形式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#回归任务"><span class="toc-number">4.</span> <span class="toc-text">回归任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归"><span class="toc-number">4.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多元线性回归"><span class="toc-number">4.2.</span> <span class="toc-text">多元线性回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类任务"><span class="toc-number">5.</span> <span class="toc-text">分类任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#对数几率回归"><span class="toc-number">5.1.</span> <span class="toc-text">对数几率回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#线性判别分析（LDA）"><span class="toc-number">5.2.</span> <span class="toc-text">线性判别分析（LDA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多分类"><span class="toc-number">5.3.</span> <span class="toc-text">多分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#一对一（OvO）"><span class="toc-number">5.3.1.</span> <span class="toc-text">一对一（OvO）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#一对其余（OvR）"><span class="toc-number">5.3.2.</span> <span class="toc-text">一对其余（OvR）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多对多（MvM）"><span class="toc-number">5.3.3.</span> <span class="toc-text">多对多（MvM）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#类别不平衡"><span class="toc-number">5.4.</span> <span class="toc-text">类别不平衡</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结尾"><span class="toc-number">6.</span> <span class="toc-text">结尾</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考链接"><span class="toc-number">7.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/20200813024012574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center)"><div id="post-info"><div id="post-title"><div class="posttitle">西瓜书笔记：第3章·线性模型</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-08-13<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-08-13</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E9%83%91%E5%86%A0%E5%BD%A7/">郑冠彧</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E9%83%91%E5%86%A0%E5%BD%A7/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.4k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 11 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><blockquote>
<p>个人情况：</p>
<ul>
<li>数学基础 -&gt; 考研数学一</li>
<li>编程基础 -&gt; 过去偏向应用开发，任务需要会兼顾前后端，nlp相关知之甚少</li>
<li>整体情况 -&gt; 小白一枚～</li>
<li>阅读目的 -&gt; 初次阅读重点在于了解理论模型构建过程</li>
</ul>
</blockquote>
<p>&emsp;&emsp;原意是通过项目推动理论学习，不过基于零基础点NLP的技能树显然是有些不自量力。在学习某入门文本分类实践中，遇到了一个感兴趣的名词——SVM支持向量机。随后找到西瓜书略读相关理论，在<code>6.4软间隔与正则化</code>和<code>6.6核方法</code>分别遇到了<code>对率损失</code>和<code>线性判别分析</code>两个概念，它们都指向了<code>第3章线性模型</code>（没想到还是个西瓜书宇宙～）。本着求知（实则无奈～）的原则，读一读哈～</p>
<h2 id="梳理框架"><a href="#梳理框架" class="headerlink" title="梳理框架"></a>梳理框架</h2><p><a href="https://img-blog.csdnimg.cn/20200813024012574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener" data-fancybox="group" data-caption="线性模型思维导图" class="fancybox"><img alt="线性模型思维导图" title="线性模型思维导图" data-src="https://img-blog.csdnimg.cn/20200813024012574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center" class="lazyload"></a></p>
<p><br></p>
<h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><blockquote>
<p>&emsp;&emsp;给定由$d$个属性描述的示例$x=(x_1;x_2;…;x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型试图学得一个通过属性的<strong>线性组合</strong>来进行预测的函数，即</p>
<script type="math/tex; mode=display">f(x)=\omega_1x_1+\omega_2x_2+...+\omega_dx_d+b</script><p>一般向量形式写成</p>
<script type="math/tex; mode=display">f(x)=\omega^Tx+b</script><p>其中$\omega=(\omega_1;\omega_2;…;\omega_d)$.$\omega$和$b$学得之后，模型就得以确定</p>
</blockquote>
<p>&emsp;&emsp;在粗读<code>SVM向量机</code>章节发现划分超平面也是通过类似的线性方程来表示，并且始终围绕着该<strong>线性组合</strong>做文章，比如在添加约束条件之后就变成了我们考研童鞋熟悉的多元微分条件极值，即基于拉格朗日乘数法思想求解$\omega$和$b$.下图是<code>SVM</code>章节的粗读思维框架，也是下一次西瓜书笔记的内容～<br><a href="https://img-blog.csdnimg.cn/20200811142730973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener" data-fancybox="group" data-caption="SVM思维框架" class="fancybox"><img alt="SVM思维框架" title="SVM思维框架" data-src="https://img-blog.csdnimg.cn/20200811142730973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center" class="lazyload"></a><br>&emsp;&emsp;因此，我们能隐约地发现线性模型形式简单易懂，也便于建模，但蕴涵着机器学习中一些重要的基本思想。而且，不难发现$\omega$能直观表达各属性在预测中的重要性，这使得线性模型由很好的<strong>可解释性</strong>。以<code>如何挑选西瓜</code>为例，可建立一个简易的线性模型$f_{好瓜}(x)=0.2\cdot x_{色泽}+0.5\cdot x_{根蒂}+0.3\cdot x_{敲声}+1$，说明可以综合考虑色泽、根蒂和敲声来判断瓜的好坏，其中根蒂最重要。</p>
<h2 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h2><p>&emsp;&emsp;给一个前提条件：给定数据集$D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}$，其中$x_i=(x_{i1};x_{i2};…;x_{id})$，$y_i\in R$.回归任务试图学得一个线性模型以尽可能准确地预测实值输出标记。接下来我们会逐渐提升情况的复杂程度。</p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>&emsp;&emsp;凡事由浅入深，化繁为简。最简单的情形显而易见，输入属性的数目只有一个（即一元）。线性回归试图学得</p>
<script type="math/tex; mode=display">f(x_i)=\omega x_i+b</script><p>使得$f(x_i)\approx y_i$.</p>
<p>&emsp;&emsp;那么问题来了，如何确定$\omega$和$b$呢？这里我们可以运用回归任务中最常用的性能度量——<strong>均方误差</strong>，即衡量$f(x)$与$y$的差别。所以我们的最终目的就是使均方误差最小化，即</p>
<script type="math/tex; mode=display">(\omega^*,b^*)=arg_{(\omega,b)}min\Sigma^m _{i=1}  (f(x_i)-y_i)^2</script><script type="math/tex; mode=display">=arg_{(\omega,b)}min\Sigma^m _{i=1} (y_i-\omega x_i-b)^2</script><p>&emsp;&emsp;这里我们介绍一种方法——<strong>最小二乘法</strong>，它是基于均方误差最小化进行模型求解的。而在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的<code>欧氏距离之和</code>最小。</p>
<p>&emsp;&emsp;求解$\omega$和$b$使$E_{(\omega,b)}=\Sigma^m _{i=1} (y_i-\omega x_i-b)^2$最小化的过程，称为<strong>线性回归模型的最小二乘“参数估计”</strong>。对$E_{(\omega,b)}$分别求$\omega$和$b$的偏导，得到</p>
<blockquote>
<p><a href="https://img-blog.csdnimg.cn/2020081301434699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="偏导" class="fancybox"><img alt="偏导" title="偏导" data-src="https://img-blog.csdnimg.cn/2020081301434699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a><br>进而令上述两式为零，得到$\omega$和$b$的最优解的闭式解<br><a href="https://img-blog.csdnimg.cn/20200813014534872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="1" class="fancybox"><img alt="1" title="1" data-src="https://img-blog.csdnimg.cn/20200813014534872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a><br><a href="https://img-blog.csdnimg.cn/20200813014550814.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" title="2" data-src="https://img-blog.csdnimg.cn/20200813014550814.png" class="lazyload"></a></p>
</blockquote>
<p><br></p>
<blockquote>
<p><strong>注</strong>：这里$E_{(\omega,b)}$是关于$\omega$和$b$的凸函数，当他关于$\omega$和$b$的导数均为零时，得到$\omega$和$b$的最优解。</p>
<p><strong>个人理解</strong>：什么是凸函数？举一个简单的例子：$f(x)=x^2$（脑海里很容易出现该函数图像～）。在高数中，我们知道如何判断函数的凹凸性，即判断二阶导的正负性。这样我们就可以很容易理解令偏导为零得到最优解。因为我们的目的是使均方误差最小化，而凸函数一阶导为零能求出极小值（想刚才那个简单的函数图像～）。这样我们就可以将线性回归的最小二乘法与高数内容联系到一起，从而更好地理解数学推导过程。</p>
</blockquote>
<h3 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h3><p>&emsp;&emsp;现在进一步推广，回到开头，样本由$d$个属性描述，此时我们试图学得</p>
<script type="math/tex; mode=display">f(x)=\omega^Tx+b</script><p>这就是多元线性回归（对应上一小节的<strong>一元</strong>）.</p>
<p>&emsp;&emsp;依旧采用最小二乘法，不过为便于讨论，把$\omega$和$b$吸收入向量形式$\omega^\Lambda=(\omega,b)$.相应，把数据集$D$表示为一个$m\times(d+1)$大小的矩阵$X$，其中每行对应于一个示例，该行前$d$个元素对应于示例的$d$个属性值，最后一个元素恒置为1，即<br><a href="https://img-blog.csdnimg.cn/20200813020852774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="3" class="fancybox"><img alt="3" title="3" data-src="https://img-blog.csdnimg.cn/20200813020852774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a></p>
<p>&emsp;&emsp;之后将以向量形式改写均方误差最小化的式子，有<br><a href="https://img-blog.csdnimg.cn/20200813021131631.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="5" class="fancybox"><img alt="5" title="5" data-src="https://img-blog.csdnimg.cn/20200813021131631.png" class="lazyload"></a></p>
<p>随后也是相同的动作——求导。得到：<br><a href="https://img-blog.csdnimg.cn/20200813021146659.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="6" class="fancybox"><img alt="6" title="6" data-src="https://img-blog.csdnimg.cn/20200813021146659.png" class="lazyload"></a></p>
<p>同样地，令其为零，即可得到最优解的闭式解，但由于涉及到矩阵逆运算，情况比较复杂，所以我们要做一个讨论。</p>
<p>&emsp;&emsp;当$X^TX$为满秩矩阵或正定矩阵（此处又涉及到线性代数的概念），再令上式为零得到</p>
<script type="math/tex; mode=display">\omega^*=(X^TX)^{-1}X^Ty</script><p>令$x^\Lambda _i=(x_i,1)$，最终得到多元回归模型</p>
<script type="math/tex; mode=display">f(x^\Lambda _i)={x^\Lambda}^T _i(X^TX)^{-1}X^Ty</script><p>&emsp;&emsp;当然，这是理想情况，现实中往往不是满秩矩阵，甚至会遇到数目超过样例数的情况（线代知识，列大于行，肯定不满秩，而且会出现多解）。我们可能求出多个解均使均方误差最小化，这时选哪一个作为输出，常用的做法是引入<strong>正则化项</strong>。</p>
<p>&emsp;&emsp;我们要明白，我们的目的始终是预测值逼近真实值，如果直接逼近可能遇到阻力，那么是否可以使预测值逼近真实值的衍生物，也就是说我们制造一个缓冲区，作为连接预测值与真实值的桥梁。书中假设我们认为<strong>示例所对应的输出标记是在指数尺度上变化</strong>，这时我们可以取对数，即预测值逼近真实值的对数。给出数学公式，</p>
<script type="math/tex; mode=display">lny=\omega^Tx+b</script><p>这就是<strong>对数线性回归</strong>（区别于对数几率回归）。形式上依旧是线性回归，但实质上已经是在求取输入空间到输出空间到非线性函数映射。<br><a href="https://img-blog.csdnimg.cn/20200813023136670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="对数线性回归示意图" class="fancybox"><img alt="对数线性回归示意图" title="对数线性回归示意图" data-src="https://img-blog.csdnimg.cn/20200813023136670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a></p>
<p>&emsp;&emsp;继续一般化，令</p>
<script type="math/tex; mode=display">y=g^{-1}(\omega^Tx+b)</script><p>这样就得到“广义线性模型”，其中$g(·)$称为“<strong>联系函数</strong>”。</p>
<h2 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h2><p>&emsp;&emsp;如何使用线性模型进行分类任务学习？</p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>&emsp;&emsp;受上一节末广义线性模型的启发，我们也可以找一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系到一起。<br>&emsp;&emsp;仅考虑二分类任务，其输出标记$y\in \{0,1\}$.考虑到线性回归模型$z=\omega^Tx+b$产生的预测值是实数，所以我们将$z$转换成0/1值，最理想的是“单位阶跃函数”<br><a href="https://img-blog.csdnimg.cn/20200813025050496.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="单位阶跃函数" class="fancybox"><img alt="单位阶跃函数" title="单位阶跃函数" data-src="https://img-blog.csdnimg.cn/20200813025050496.png" class="lazyload"></a></p>
<p>&emsp;&emsp;不过显而易见，单位阶跃函数不连续，不能直接作为联系函数。这时我们可以找一个替代的函数，近似单位阶跃函数的性质，而对数几率函数就满足我们的需求</p>
<script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}</script><p>下图为单位阶跃函数与对数几率函数的对比图，注意输出值在$z=0$附近变化很陡。<br><a href="https://img-blog.csdnimg.cn/20200813025221731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="对比" class="fancybox"><img alt="对比" title="对比" data-src="https://img-blog.csdnimg.cn/20200813025221731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a></p>
<center><font size="2">单位阶跃函数与对数几率函数</font></center>

<p><br></p>
<blockquote>
<p>&emsp;&emsp;我们发现，这时一个包裹着回归外衣的分类学习方法。这种方法有很多优点，比如它直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题。</p>
</blockquote>
<p>&emsp;&emsp;自然而然，我们要确定$\omega$和$b$。书中给出了“极大似然法”，类似于我们概率论与数理统计中参数估计章节的最大似然估计法。并且在求解似然函数部分举例运用了梯度下降法和牛顿法得到最优解。具体数学推导部分可参考书籍（其实没推明白～）。</p>
<h3 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h3><blockquote>
<p>&emsp;&emsp;思想比较朴素：给定训练样例集，设法将样例投影到一条直线上，使同类样例投影点尽可能接近、异类样例投影点尽可能远离；对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。<br><a href="https://img-blog.csdnimg.cn/2020081303105834.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="LDA示意图" class="fancybox"><img alt="LDA示意图" title="LDA示意图" data-src="https://img-blog.csdnimg.cn/2020081303105834.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a></p>
</blockquote>
<h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>&emsp;&emsp;基本思路是“拆解法”，即将多分类任务拆若干个二分类任务求解。具体来说，先对问题进行拆分，然后对拆出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。书中该章节主要介绍拆分策略。</p>
<p>&emsp;&emsp;给定数据集$D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}，y_i\in \{C_1,C_2,…,C_N\}$</p>
<h4 id="一对一（OvO）"><a href="#一对一（OvO）" class="headerlink" title="一对一（OvO）"></a>一对一（OvO）</h4><blockquote>
<p>&emsp;&emsp;将这N个类别两两配对，例如OvO将为区分类别$C_i$和$C_j$训练一个分类器，该分类器把$D$中的$C_i$类样例作为正例，$C_j$类样例作为反例。在测试阶段，新样本将同时提交给所有分类器，然后得到$N(N-1)/2$个结果，最终结果通过投票产生：即把被预测得最多的类别作为最终分类结果。后面会给出示意图。</p>
</blockquote>
<h4 id="一对其余（OvR）"><a href="#一对其余（OvR）" class="headerlink" title="一对其余（OvR）"></a>一对其余（OvR）</h4><blockquote>
<p>&emsp;&emsp;每次将一个类的样例作为正例、所有其他类的法样例作为反例来训练$N$个分类器，在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则考虑各个分类器的预测置信度，选择置信度最大的类别标记作为分类结果。</p>
</blockquote>
<p>&emsp;&emsp;下图为OvO与OvR的过程图。</p>
<p><a href="https://img-blog.csdnimg.cn/20200813032539697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="OvO与OvR示意图" class="fancybox"><img alt="OvO与OvR示意图" title="OvO与OvR示意图" data-src="https://img-blog.csdnimg.cn/20200813032539697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a></p>
<p><center><font size="2">OvO与OvR示意图</font></center><br><br></p>
<h4 id="多对多（MvM）"><a href="#多对多（MvM）" class="headerlink" title="多对多（MvM）"></a>多对多（MvM）</h4><p>&emsp;&emsp;每次将若干个类作为正类，若干个其他类作为反类。MvM的正、反类构造必须有特殊的设计，书中介绍一种常用的技术：<strong>纠错输出码</strong>（ECOC）。</p>
<blockquote>
<ul>
<li>编码：对$N$个类别做$M$次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集；这样一共产生$M$个训练集，可训练出$M$个分类器。</li>
<li>解码：$M$个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这个预测编码与每个类别各自的编码进行比较，返回其中举例最小的类别作为最终预测结果。</li>
</ul>
</blockquote>
<p><a href="https://img-blog.csdnimg.cn/20200813033530838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="ECOC" class="fancybox"><img alt="ECOC" title="ECOC" data-src="https://img-blog.csdnimg.cn/20200813033530838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70" class="lazyload"></a></p>
<h3 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h3><p>&emsp;&emsp;让我们回到现实，前面介绍的分类学习法都有一个共同的基本假设，即<strong>不同类别的训练样例数目相当</strong>。如果差别很大呢？例如有998个反例，2个正例，那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度，显然这样的学习器没有价值，因为它不能预测出任何正例。</p>
<p>&emsp;&emsp;基于线性分类器的角度讨论，这里有一个类别不平衡学习的一个基本策略——“再缩放”。</p>
<script type="math/tex; mode=display">\frac{y'}{1-y'}=\frac{y}{1-y}\times \frac{m^-}{m^+}</script><p>其中$m^-$表示反例数目，$m^+$表示正例数目，$y$就是对数几率回归里的正例可能性。通常我们假设训练集时真实样本总体的无偏样本（参考参数估计的无偏估计）。</p>
<p>&emsp;&emsp;再缩放的思想简单，但实际操作很复杂，主要是因为“<strong>训练集时真实样本总体的无偏采样</strong>”这个假设往往不成立，也就是说，我们未必能有效基于训练集观测几率来推断真实几率。现有技术有三类做法：欠采样、过采样、阈值移动。各方法的过程以及优缺点对比见书～</p>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>&emsp;&emsp;能力有限，还未作出完整有效的思路输出，且对书中的公式推导存在许多疑惑，仍需努力。<br>&emsp;&emsp;未来还会随着理解的加深不断修改文章。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><blockquote>
<p><a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">周志华.机器学习[M]:清华大学出版社,2016</a></p>
</blockquote>
<p><br></p>
<blockquote>
<p>如果有错误或者不严谨的地方，请务必给予指正，十分感谢。<br>本人blog：<a href="http://breadhunter.gitee.io" target="_blank" rel="noopener">http://breadhunter.gitee.io</a></p>
</blockquote>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习    </a><a class="post-meta__tags" href="/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/">西瓜书    </a></div><div class="post_share"></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/08/15/SSH%E7%AE%80%E4%BE%BF%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95+%E9%80%9A%E8%BF%87SSH%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"><img class="prev_cover lazyload" data-src="https://img-blog.csdnimg.cn/20200815094201473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>SSH简便远程登录+通过SSH从服务器下载文件</span></div></a></div><div class="next-post pull_right"><a href="/2020/08/11/%E3%80%90%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E3%80%91%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9CCapsNet/"><img class="next_cover lazyload" data-src="https://img-blog.csdnimg.cn/20200402110030173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1YW45NTAyMDU=,size_16,color_FFFFFF,t_70" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>【自我学习】胶囊网络CapsNet</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/08/15/损失函数/" title="损失函数"><img class="relatedPosts_cover lazyload"data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-15</div><div class="relatedPosts_title">损失函数</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/08/机器学习（1）/" title="机器学习（1）"><img class="relatedPosts_cover lazyload"data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-08</div><div class="relatedPosts_title">机器学习（1）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/09/网站识别方案/" title="网站识别方案"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200809154715396.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc3Nzg0MQ==,size_16,color_FFFFFF,t_70#pic_center"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-09</div><div class="relatedPosts_title">网站识别方案</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/31/论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解/" title="论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.3/img/周郴莲/7.31/图片1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-31</div><div class="relatedPosts_title">论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/传统的机器学习方法——决策树（上）/" title="传统的机器学习方法——决策树（上）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">传统的机器学习方法——决策树（上）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/24/从设计循环不变量浅谈快速排序/" title="从设计循环不变量浅谈快速排序"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/zhengguanyu/PhotoRepos@v1.0/img/quickThiedSort.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-24</div><div class="relatedPosts_title">从设计循环不变量浅谈快速排序</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 东北石油大学智能技术与自然语言处理实验室</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我们团队的博客！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>