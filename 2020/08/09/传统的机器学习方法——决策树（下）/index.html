<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>传统的机器学习方法——决策树（下） | 东北石油大学智能技术与自然语言处理实验室</title><meta name="description" content="传统的机器学习方法——决策树（下）"><meta name="keywords" content="深度学习,算法,自然语言处理"><meta name="author" content="东北石油大学智能技术与自然语言处理实验室"><meta name="copyright" content="东北石油大学智能技术与自然语言处理实验室"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/ava.jpg"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="传统的机器学习方法——决策树（下）"><meta name="twitter:description" content="传统的机器学习方法——决策树（下）"><meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><meta property="og:type" content="article"><meta property="og:title" content="传统的机器学习方法——决策树（下）"><meta property="og:url" content="https://nlplab406.github.io/2020/08/09/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/"><meta property="og:site_name" content="东北石油大学智能技术与自然语言处理实验室"><meta property="og:description" content="传统的机器学习方法——决策树（下）"><meta property="og:image" content="https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://nlplab406.github.io/2020/08/09/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/"><link rel="prev" title="网站识别方案" href="https://nlplab406.github.io/2020/08/09/%E7%BD%91%E7%AB%99%E8%AF%86%E5%88%AB%E6%96%B9%E6%A1%88/"><link rel="next" title="机器学习（1）" href="https://nlplab406.github.io/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">东北石油大学智能技术与自然语言处理实验室</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/ava.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">17</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">26</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#决策树的剪枝"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">决策树的剪枝</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#剪枝介绍"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">剪枝介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#损失函数"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">损失函数</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#定义"><span class="toc_mobile_items-number">1.2.1.</span> <span class="toc_mobile_items-text">定义</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#损失函数的优点"><span class="toc_mobile_items-number">1.2.2.</span> <span class="toc_mobile_items-text">损失函数的优点</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#剪枝算法"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">剪枝算法</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#CART算法"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">CART算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#算法介绍"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">算法介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#CART生成"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">CART生成</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#介绍"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#回归树的生成"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text">回归树的生成</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#最小二乘回归树生成算法"><span class="toc_mobile_items-number">2.2.2.1.</span> <span class="toc_mobile_items-text">最小二乘回归树生成算法</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#分类树的生成"><span class="toc_mobile_items-number">2.2.3.</span> <span class="toc_mobile_items-text">分类树的生成</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#基尼指数"><span class="toc_mobile_items-number">2.2.3.1.</span> <span class="toc_mobile_items-text">基尼指数</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#CART生成算法"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">CART生成算法</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#CART剪枝"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">CART剪枝</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树的剪枝"><span class="toc-number">1.</span> <span class="toc-text">决策树的剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#剪枝介绍"><span class="toc-number">1.1.</span> <span class="toc-text">剪枝介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#定义"><span class="toc-number">1.2.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数的优点"><span class="toc-number">1.2.2.</span> <span class="toc-text">损失函数的优点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#剪枝算法"><span class="toc-number">1.3.</span> <span class="toc-text">剪枝算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CART算法"><span class="toc-number">2.</span> <span class="toc-text">CART算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法介绍"><span class="toc-number">2.1.</span> <span class="toc-text">算法介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART生成"><span class="toc-number">2.2.</span> <span class="toc-text">CART生成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#介绍"><span class="toc-number">2.2.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回归树的生成"><span class="toc-number">2.2.2.</span> <span class="toc-text">回归树的生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#最小二乘回归树生成算法"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">最小二乘回归树生成算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类树的生成"><span class="toc-number">2.2.3.</span> <span class="toc-text">分类树的生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#基尼指数"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">基尼指数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART生成算法"><span class="toc-number">2.3.</span> <span class="toc-text">CART生成算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CART剪枝"><span class="toc-number">3.</span> <span class="toc-text">CART剪枝</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70)"><div id="post-info"><div id="post-title"><div class="posttitle">传统的机器学习方法——决策树（下）</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-08-09<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-08-09</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%92%8C%E5%A9%B7%E5%A9%B7/">和婷婷</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%92%8C%E5%A9%B7%E5%A9%B7/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 6 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><h2 id="剪枝介绍"><a href="#剪枝介绍" class="headerlink" title="剪枝介绍"></a>剪枝介绍</h2><p>&emsp;&emsp;在决策树学习中将已生成的树进行简化的过程称为剪枝。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型,这就是决策树的剪枝。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>&emsp;&emsp;决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶结点个数为 |T|，t 是树 T 的叶结点，该叶结点有 Nt 个样本点，其中 k 类的样本点有 Ntk 个，k=1,2,…,K, Ht(T) 为叶结点 t 上的经验熵，α&gt;=0为参数，则决策树学习的损失函数可以定义为</p>
<p><a href="https://img-blog.csdnimg.cn/2020080916260667.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="损失函数" class="fancybox"><img alt="损失函数" data-src="https://img-blog.csdnimg.cn/2020080916260667.png" class="lazyload" title="损失函数"></a></p>
<p>&emsp;&emsp;其中经验熵为：</p>
<p><a href="https://img-blog.csdnimg.cn/20200809162710517.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="经验熵" class="fancybox"><img alt="经验熵" data-src="https://img-blog.csdnimg.cn/20200809162710517.png" class="lazyload" title="经验熵"></a></p>
<p>&emsp;&emsp;在损失函数中，将式子右端的第1项记作</p>
<p><a href="https://img-blog.csdnimg.cn/20200809162849274.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="1" class="fancybox"><img alt="1" data-src="https://img-blog.csdnimg.cn/20200809162849274.png" class="lazyload" title="1"></a></p>
<p>&emsp;&emsp;这时有</p>
<p> <a href="https://img-blog.csdnimg.cn/20200809162926637.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" data-src="https://img-blog.csdnimg.cn/20200809162926637.png" class="lazyload" title="2"></a></p>
<p>&emsp;&emsp;上式中，C(T) 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T| 表示模型复杂度，参数 α&gt;=0 控制两者之间的影响。较大的促使选择较简单的模型(树)，较小的 α 促使选择较复杂的模型(树)。α=0 意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。</p>
<h3 id="损失函数的优点"><a href="#损失函数的优点" class="headerlink" title="损失函数的优点"></a>损失函数的优点</h3><p>&emsp;&emsp;剪枝，就是当 α 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 α 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好，损失函数正好表示了对两者的平衡。</p>
<h2 id="剪枝算法"><a href="#剪枝算法" class="headerlink" title="剪枝算法"></a>剪枝算法</h2><p>&emsp;&emsp;输入：生成算法产生的整个树 T，参数 α；<br>&emsp;&emsp;输出：修建后的子树 Tα。</p>
<ol>
<li>计算每个结点的经验熵；</li>
<li>递归地从树的叶结点向上回缩。设一组叶结点回缩到父结点之前与之后的整体树分别为 Tb 与 Ta ，其对应的损失函数值分别时 Cα(Tb) 与 Cα(Ta) ，如果 Cα(Ta)&lt;=Cα(Tb) ,则进行剪枝，即将父结点变成新的叶结点。  </li>
<li>返回2，直至不能继续为止，得到损失函数最小的子树 Tα。</li>
</ol>
<p><a href="https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="剪枝" class="fancybox"><img alt="剪枝" data-src="https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="剪枝"></a></p>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><p>&emsp;&emsp;CART算法由一下两步组成：</p>
<ol>
<li>决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大；</li>
<li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。</li>
</ol>
<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>&emsp;&emsp;决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。</p>
<h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><p>&emsp;&emsp;假设 X 与 Y 分别为输入和输出变量，并且 Y 是连续变量，给定训练数据集</p>
<p><a href="https://img-blog.csdnimg.cn/20200809170705517.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="训练数据集" class="fancybox"><img alt="训练数据集" data-src="https://img-blog.csdnimg.cn/20200809170705517.png" class="lazyload" title="训练数据集"></a></p>
<p>&emsp;&emsp;考虑如何生成回归树。</p>
<p>&emsp;&emsp;一个回归树对应着输入空间的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为 M 个单元 R1，R2 ，…..,Rm，并且在每个单元 Rm 上有一个固定的输出值 Cm,于是回归树模型可表示为</p>
<p><a href="https://img-blog.csdnimg.cn/20200809171022711.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="回归树模型" class="fancybox"><img alt="回归树模型" data-src="https://img-blog.csdnimg.cn/20200809171022711.png" class="lazyload" title="回归树模型"></a></p>
<p>&emsp;&emsp;当输入空间的划分确定时，可以用平方误差来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。<br>&emsp;&emsp;平方误差如下:</p>
<p><a href="https://img-blog.csdnimg.cn/20200809171247348.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="平方误差" class="fancybox"><img alt="平方误差" data-src="https://img-blog.csdnimg.cn/20200809171247348.png" class="lazyload" title="平方误差"></a></p>
<h4 id="最小二乘回归树生成算法"><a href="#最小二乘回归树生成算法" class="headerlink" title="最小二乘回归树生成算法"></a>最小二乘回归树生成算法</h4><p>&emsp;&emsp;输入：训练数据集 D；<br>&emsp;&emsp;输出：回归树  f(x)。<br>&emsp;&emsp;在训练数据集所在的输入空间中，递归地将每个区域划分为两个区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<ol>
<li>选择最优切分变量 j 与切分点 s ，求解：</li>
</ol>
<p><a href="https://img-blog.csdnimg.cn/20200809171931255.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="1" class="fancybox"><img alt="1" data-src="https://img-blog.csdnimg.cn/20200809171931255.png" class="lazyload" title="1"></a></p>
<p>遍历变量 j ，对固定的切分变量 j 扫描切分点 s ，选择使上式达到最小值的对  (j,s)。</p>
<ol>
<li>用选定的对 (j,s) 划分区域并决定相应的输出值：</li>
</ol>
<p><a href="https://img-blog.csdnimg.cn/20200809172357678.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" data-src="https://img-blog.csdnimg.cn/20200809172357678.png" class="lazyload" title="2"></a></p>
<ol>
<li>继续对两个子区域调用步骤 1 ，2 ，直至满足停止条件。</li>
<li>将输入空间划分为 M 个区域 R1,R2,…,Rm,生成决策树：</li>
</ol>
<p><a href="https://img-blog.csdnimg.cn/20200809172747220.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="4" class="fancybox"><img alt="4" data-src="https://img-blog.csdnimg.cn/20200809172747220.png" class="lazyload" title="4"></a></p>
<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>&emsp;&emsp;分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>&emsp;&emsp;分类问题中，假设有 K 个类，样本点属于第 k 类的概率为 Pk,则概率分布的基尼指数定义为</p>
<p><a href="https://img-blog.csdnimg.cn/20200809173348840.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" data-src="https://img-blog.csdnimg.cn/20200809173348840.png" class="lazyload" title="2"></a></p>
<p>&emsp;&emsp;对于二类分类问题，若样本点属于第1个类的概率是 p，则概率分布的基尼指数为</p>
<p><a href="https://img-blog.csdnimg.cn/20200809173512505.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" data-src="https://img-blog.csdnimg.cn/20200809173512505.png" class="lazyload" title="2"></a></p>
<p>&emsp;&emsp;对于给定的样本集合 D，其基尼指数为</p>
<p><a href="https://img-blog.csdnimg.cn/20200809173659351.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="3" class="fancybox"><img alt="3" data-src="https://img-blog.csdnimg.cn/20200809173659351.png" class="lazyload" title="3"></a></p>
<p>&emsp;&emsp;这里，Ck是 D 中属于第 k 类的样本子集，K 是类的个数。<br>&emsp;&emsp;如果样本集合 D 根据特征 A 是否取某一可能 α 被分割成 D1和 D2 两部分，即</p>
<p><a href="https://img-blog.csdnimg.cn/20200809174539445.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="3" class="fancybox"><img alt="3" data-src="https://img-blog.csdnimg.cn/20200809174539445.png" class="lazyload" title="3"></a></p>
<p>&emsp;&emsp;则在特征 A 的条件下，集合 D 的基尼指数定于为</p>
<p><a href="https://img-blog.csdnimg.cn/20200809174646545.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="6" class="fancybox"><img alt="6" data-src="https://img-blog.csdnimg.cn/20200809174646545.png" class="lazyload" title="6"></a></p>
<p>&emsp;&emsp;基尼指数表示集合 D 的不确定性，基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。</p>
<h2 id="CART生成算法"><a href="#CART生成算法" class="headerlink" title="CART生成算法"></a>CART生成算法</h2><p>&emsp;&emsp;输入：训练数据集 D ，停止计算的条件；<br>&emsp;&emsp;输出： CART 决策树。<br>&emsp;&emsp;根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：</p>
<ol>
<li><p>设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每一个特征，对其可能取得每个值a，根据样本点对A=a的测试为“是”或“否”将D分割成两部分，利用基尼指数公式计算A=a时的基尼指数。</p>
</li>
<li><p>对所有可能的特征A以及他们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最有特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</p>
</li>
<li><p>对两个子结点递归地调用1,2，直到满足停止条件。</p>
</li>
<li><p>生成CART决策树。</p>
</li>
</ol>
<h1 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h1><p>&emsp;&emsp;CART剪枝算法：<br>&emsp;&emsp;输入：CART算法生成的决策树 T0；<br>&emsp;&emsp;输出：最优决策树 Tα。</p>
<ul>
<li>（1）设k=0, T=T0</li>
<li>（2）设α=+∞</li>
<li><p>（3）自下而上地对各个内部结点t计算C(Tt)，|Tt| 以及</p>
<p><a href="https://img-blog.csdnimg.cn/20200809181004221.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="5" class="fancybox"><img alt="5" data-src="https://img-blog.csdnimg.cn/20200809181004221.png" class="lazyload" title="5"></a></p>
</li>
</ul>
<p>&emsp;&emsp;这里，Tt 表示以 t 为根结点的子树，C(Tt) 是对训练数据的预测误差，|Tt| 是 Tt 的叶结点个数。</p>
<ul>
<li>（4）自上而下地访问内部结点 t ，如果有 g(t) = α ,进行剪枝，并对叶结点 t 以多数表决法表决其类，得到树 T 。</li>
<li>（5）设 k=k+1, αk=α, Tk=T。</li>
<li>（6）如果 T 不是由根结点单独构成的树，则回到步骤4。</li>
<li>（7）采用交叉验证法在子树序列 T0,T1,…,Tn 中选取最优子树Tα。</li>
</ul>
<blockquote>
<p>参考文献：李航. 统计学习方法[M]. 北京：清华大学出版社，2012</p>
</blockquote>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习    </a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a></div><div class="post_share"></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/08/09/%E7%BD%91%E7%AB%99%E8%AF%86%E5%88%AB%E6%96%B9%E6%A1%88/"><img class="prev_cover lazyload" data-src="https://img-blog.csdnimg.cn/20200809154715396.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc3Nzg0MQ==,size_16,color_FFFFFF,t_70#pic_center" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>网站识别方案</span></div></a></div><div class="next-post pull_right"><a href="/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/"><img class="next_cover lazyload" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/1.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>机器学习（1）</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/08/02/传统的机器学习方法——决策树（上）/" title="传统的机器学习方法——决策树（上）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">传统的机器学习方法——决策树（上）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/命名实体识别简介（一）/" title="命名实体识别简介（一）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802093111103.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hhcHB5ZXZlcnl5ZGF5,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">命名实体识别简介（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/知识图谱/" title="知识图谱"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.4/img/张明磊/8.2/图片2.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">知识图谱</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/31/论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解/" title="论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.3/img/周郴莲/7.31/图片1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-31</div><div class="relatedPosts_title">论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks/" title="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/08/机器学习（1）/" title="机器学习（1）"><img class="relatedPosts_cover lazyload"data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-08</div><div class="relatedPosts_title">机器学习（1）</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 东北石油大学智能技术与自然语言处理实验室</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我们团队的博客！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>