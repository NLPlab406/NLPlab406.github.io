<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks | 东北石油大学智能技术与自然语言处理实验室</title><meta name="description" content="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks"><meta name="keywords" content="深度学习,论文,自然语言处理,情感分析,卷积神经网络"><meta name="author" content="东北石油大学智能技术与自然语言处理实验室"><meta name="copyright" content="东北石油大学智能技术与自然语言处理实验室"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/ava.jpg"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks"><meta name="twitter:description" content="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks"><meta property="og:url" content="https://nlplab406.github.io/2020/08/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAspect-based%20Sentiment%20Classification%20with%20Aspect-specific%20Graph%20Convolutional%20Networks/"><meta property="og:site_name" content="东北石油大学智能技术与自然语言处理实验室"><meta property="og:description" content="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://nlplab406.github.io/2020/08/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAspect-based%20Sentiment%20Classification%20with%20Aspect-specific%20Graph%20Convolutional%20Networks/"><link rel="prev" title="分类任务评价指标" href="https://nlplab406.github.io/2020/08/02/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"><link rel="next" title="传统的机器学习方法——决策树（上）" href="https://nlplab406.github.io/2020/08/02/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">东北石油大学智能技术与自然语言处理实验室</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/ava.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">17</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">26</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Introduction"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Introduction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Graph-Convolutional-Networks"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">Graph Convolutional Networks</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Aspect-specific-Graph-Convolutional-Network"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Aspect-specific Graph Convolutional Network</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Embedding-and-Bidirectional-LSTM"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">Embedding and Bidirectional LSTM</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Obtaining-Aspect-oriented-Features（获得面向方面的特征）"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">Obtaining Aspect-oriented Features（获得面向方面的特征）</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Graph-Convolution-over-Dependency-Trees（依赖树上的图卷积）"><span class="toc_mobile_items-number">3.2.1.</span> <span class="toc_mobile_items-text">Graph Convolution over Dependency Trees（依赖树上的图卷积）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Aspect-specific-Masking"><span class="toc_mobile_items-number">3.2.2.</span> <span class="toc_mobile_items-text">Aspect-specific Masking</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Aspect-aware-Attention"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">Aspect-aware Attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Sentiment-Classification"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">Sentiment Classification</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Training"><span class="toc_mobile_items-number">3.5.</span> <span class="toc_mobile_items-text">Training</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Experiments"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">Experiments</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Datasets-and-Experimental-Settings"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">Datasets and Experimental Settings</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Models-for-Comparison"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">　Models for Comparison</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Results"><span class="toc_mobile_items-number">4.3.</span> <span class="toc_mobile_items-text">Results</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Ablation-Study"><span class="toc_mobile_items-number">4.4.</span> <span class="toc_mobile_items-text">Ablation Study</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Case-Study"><span class="toc_mobile_items-number">4.5.</span> <span class="toc_mobile_items-text">Case Study</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Discussion"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">Discussion</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Investigation-on-the-Impact-of-GCN（GCN层影响的调查）"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">Investigation on the Impact of GCN（GCN层影响的调查）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Investigation-on-the-Effect-of-Multiple-Aspects"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">Investigation on the Effect of Multiple Aspects</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Related-Work"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">Related Work</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Conclusions-and-Future-Work"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">Conclusions and Future Work</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Graph-Convolutional-Networks"><span class="toc-number">2.</span> <span class="toc-text">Graph Convolutional Networks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Aspect-specific-Graph-Convolutional-Network"><span class="toc-number">3.</span> <span class="toc-text">Aspect-specific Graph Convolutional Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding-and-Bidirectional-LSTM"><span class="toc-number">3.1.</span> <span class="toc-text">Embedding and Bidirectional LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Obtaining-Aspect-oriented-Features（获得面向方面的特征）"><span class="toc-number">3.2.</span> <span class="toc-text">Obtaining Aspect-oriented Features（获得面向方面的特征）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Graph-Convolution-over-Dependency-Trees（依赖树上的图卷积）"><span class="toc-number">3.2.1.</span> <span class="toc-text">Graph Convolution over Dependency Trees（依赖树上的图卷积）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Aspect-specific-Masking"><span class="toc-number">3.2.2.</span> <span class="toc-text">Aspect-specific Masking</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Aspect-aware-Attention"><span class="toc-number">3.3.</span> <span class="toc-text">Aspect-aware Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sentiment-Classification"><span class="toc-number">3.4.</span> <span class="toc-text">Sentiment Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training"><span class="toc-number">3.5.</span> <span class="toc-text">Training</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Experiments"><span class="toc-number">4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Datasets-and-Experimental-Settings"><span class="toc-number">4.1.</span> <span class="toc-text">Datasets and Experimental Settings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Models-for-Comparison"><span class="toc-number">4.2.</span> <span class="toc-text">　Models for Comparison</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results"><span class="toc-number">4.3.</span> <span class="toc-text">Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ablation-Study"><span class="toc-number">4.4.</span> <span class="toc-text">Ablation Study</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Case-Study"><span class="toc-number">4.5.</span> <span class="toc-text">Case Study</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Discussion"><span class="toc-number">5.</span> <span class="toc-text">Discussion</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Investigation-on-the-Impact-of-GCN（GCN层影响的调查）"><span class="toc-number">5.1.</span> <span class="toc-text">Investigation on the Impact of GCN（GCN层影响的调查）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Investigation-on-the-Effect-of-Multiple-Aspects"><span class="toc-number">5.2.</span> <span class="toc-text">Investigation on the Effect of Multiple Aspects</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Related-Work"><span class="toc-number">6.</span> <span class="toc-text">Related Work</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Conclusions-and-Future-Work"><span class="toc-number">7.</span> <span class="toc-text">Conclusions and Future Work</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png)"><div id="post-info"><div id="post-title"><div class="posttitle">论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-08-02<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-08-02</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%B1%E9%91%AB%E6%B5%B7/">朱鑫海</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%B1%E9%91%AB%E6%B5%B7/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/">情感分析</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">4.8k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 15 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="1" class="fancybox"><img alt="1" title="1" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png" class="lazyload"></a></p>
<p>本文是关于方面级情感分析的一篇论文，于2019年发表在EMNLP上，下面是对这篇论文的详细介绍。</p>
<ul>
<li><a href="#id1">1 Introduction</a></li>
<li><a href="#id2">2 Graph Convolutional Networks</a></li>
<li><a href="#id3">3 Aspect-specific Graph Convolutional Network</a><ul>
<li><a href="#id31">3.1 Embedding and Bidirectional LSTM</a></li>
<li><a href="#id32">3.2 Obtaining Aspect-oriented Features（获得面向方面的特征）</a><ul>
<li><a href="#id321">3.2.1 Graph Convolution over Dependency Trees（依赖树上的图卷积）</a></li>
<li><a href="#id322">3.2.2 Aspect-specific Masking</a></li>
</ul>
</li>
<li><a href="#id33">3.3 Aspect-aware Attention</a></li>
<li><a href="#id34">3.4 Sentiment Classification</a></li>
<li><a href="#id35">3.5 Training</a></li>
</ul>
</li>
<li><a href="#id4">4 Experiments</a><ul>
<li><a href="#id41">4.1 Datasets and Experimental Settings</a></li>
<li><a href="#id42">4.2 Models for Comparison</a></li>
<li><a href="#id43">4.3 Results</a></li>
<li><a href="#id44">4.4 Ablation Study</a></li>
<li><a href="#id45">4.5 Case Study</a></li>
</ul>
</li>
<li><a href="#id5">5 Discussion</a><ul>
<li><a href="#id51">5.1 Investigation on the Impact of GCN（GCN层影响的调查）</a></li>
<li><a href="#id52">5.2 Investigation on the Effect of Multiple Aspects</a></li>
</ul>
</li>
<li><a href="#id6">6 Related Work</a></li>
<li><a href="#id7">7 Conclusions and Future Work</a></li>
</ul>
<center>Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</center>

<center>基于方面的特定方面图卷积网络情感分类</center>

<p>&emsp;&emsp;注意机制和卷积神经网络由于其固有的方面和上下文词的语义对齐能力，被广泛应用于基于方面的情感分类。然而，这些模型缺乏一种机制来解释相关的句法约束和长距离单词依赖性，并且可能错误地将句法上不相关的上下文单词识别为判断方面情感的线索。为了解决这个问题，<strong>我们建议在句子的依存关系树上建立一个图卷积网络(GCN)，以利用句法信息和单词依存关系。在此基础上，提出了一种新的特定方面的情感分类框架</strong>。在三个基准测试集合上的实验表明，我们提出的模型具有与一系列最先进的模型1相当的有效性，并且进一步<strong>证明了语法信息和长距离单词依赖都被图形卷积结构恰当地捕获</strong>。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><span id="id1">Introduction</span></h1><p>&emsp;&emsp;尽管基于注意力的模型很有希望，但是它们不足以捕捉上下文词和句子中方面之间的句法依赖性。因此，<strong>当前的关注机制可能导致给定的方面错误地将句法上不相关的上下文词作为描述符</strong>（限制1）。一个方面的情感通常由关键短语而不是单个单词决定。尽管如此，基于CNN的模型只能通过对单词序列进行卷积操作将多单词特征感知为连续单词，但<strong>不足以确定由彼此不相邻的多个单词所表示的情感</strong>（限制2）。</p>
<p>&emsp;&emsp;在本文中，我们旨在通过使用图卷积网络（GCN）解决上述两个局限性（Kipf和Welling，2017）。 GCN具有多层体系结构，每一层都使用直接邻居的特征来编码和更新图中节点的表示。通过引用语法相关性树，GCN可能能够将语法相关的单词绘制到目标方面，并利用GCN层利用远程多单词关系和语法信息。</p>
<p>&emsp;&emsp;据我们所知，它是第一个基于GCN的基于方面的情感分类模型。</p>
<p>&emsp;&emsp;ASGCN从双向长期短期记忆网络（LSTM）层开始，以捕获有关单词顺序的上下文信息。为了获得特定于方面的特征，在LSTM输出的顶部实现了多层图卷积结构，然后是masking机制，该机制可过滤掉非方面的单词并仅保留高级方面的特定特征。特定于方面的特征被反馈到LSTM输出，以检索有关该方面的信息性特征，然后将其用于预测基于方面的情感。</p>
<p>&emsp;&emsp;我们的贡献如下：</p>
<p>•我们<strong>建议利用句子中的句法依存结构，并解决基于方面的情感分类的长距离多词依存问题。</strong></p>
<p>•我们<strong>认为图卷积网络（GCN）适合我们的目的，并提出了一种新颖的方面特定的GCN模型。</strong><br>•广泛的实验结果<strong>证明了利用句法信息和远程单词依存关系的重要性，并证明了我们的模型在基于方面的情感分类中捕获和利用它们的有效性。</strong></p>
<h1 id="Graph-Convolutional-Networks"><a href="#Graph-Convolutional-Networks" class="headerlink" title="Graph Convolutional Networks"></a><span id="id2">Graph Convolutional Networks</span></h1><p>&emsp;&emsp;GCN可以看作是常规CNN的改编，用于对非结构化数据的本地信息进行编码。对于具有k个节点的给定图，通过枚举该图获得邻接矩阵A。为方便起见，我们将节点i的第l层的输出表示为hli，其中h0表示节点i的初始状态。对于L层GCN，l∈[1,2，···，L]，hLi是节点i的最终状态。对节点表示进行操作的图卷积可写为：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/2.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" title="2" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/2.png" class="lazyload"></a></p>
<p>&emsp;&emsp;由于图卷积过程仅编码直接邻居的信息，因此图中的节点只能受到L层GCN中L步内的相邻节点的影响。在这样，<strong>在句子的依赖树上的图卷积为句子中的一个方面提供了句法约束，以基于句法距离识别描述性词。此外，GCN能够处理用非连续词描述方面的极性的情况，因为依赖树上的GCN会将非连续词收集到较小的范围内，并通过图卷积适当地聚集其特征。因此，我们受到启发，采用GCN来利用句法信息和远程单词依存关系进行基于方面的情感分类。</strong></p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/3.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="3" class="fancybox"><img alt="3" title="3" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/3.png" class="lazyload"></a></p>
<h1 id="Aspect-specific-Graph-Convolutional-Network"><a href="#Aspect-specific-Graph-Convolutional-Network" class="headerlink" title="Aspect-specific Graph Convolutional Network"></a><span id="id3">Aspect-specific Graph Convolutional Network</span></h1><h2 id="Embedding-and-Bidirectional-LSTM"><a href="#Embedding-and-Bidirectional-LSTM" class="headerlink" title="Embedding and Bidirectional LSTM"></a><span id="id31">Embedding and Bidirectional LSTM</span></h2><p>&emsp;&emsp;给定一个n词句子c = {wc 1，wc 2，···，wcτ+ 1，···，wcτ+ m，···，wc n-1，wc n}包含相应的从第（τ+ 1）个标记开始的m个单词方面，我们将每个单词标记嵌入具有嵌入矩阵E∈R | V |×de的低维实值向量空间（Bengio等，2003），其中| V |是词汇量的大小，表示单词嵌入的维数。利用句子的词嵌入，构造双向LSTM来产生隐藏状态向量Hc = {hc 1，hc 2，···，hcτ+ 1，···，hcτ+ m，···，hc n−1，hc n}，其中hct∈R2dh表示从双向LSTM开始的时间步t处的隐藏状态向量，而dh是单向LSTM输出的隐藏状态向量的维数。</p>
<h2 id="Obtaining-Aspect-oriented-Features（获得面向方面的特征）"><a href="#Obtaining-Aspect-oriented-Features（获得面向方面的特征）" class="headerlink" title="Obtaining Aspect-oriented Features（获得面向方面的特征）"></a><span id="id32">Obtaining Aspect-oriented Features（获得面向方面的特征）</span></h2><p>&emsp;&emsp;与一般的情感分类不同，基于方面的情感分类的目标是从方面的角度判断情感，因此<strong>需要面向方面的特征提取策略</strong>。在这项研究中，<strong>我们通过在句子的句法依赖树上应用多层图卷积，并在其顶部强加一个特定于方面的屏蔽层，来获得面向方面的特征</strong>。</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/4.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="4" class="fancybox"><img alt="4" title="4" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/4.png" class="lazyload"></a></p>
<h3 id="Graph-Convolution-over-Dependency-Trees（依赖树上的图卷积）"><a href="#Graph-Convolution-over-Dependency-Trees（依赖树上的图卷积）" class="headerlink" title="Graph Convolution over Dependency Trees（依赖树上的图卷积）"></a><span id="id321">Graph Convolution over Dependency Trees（依赖树上的图卷积）</span></h3><p>&emsp;&emsp;为了解决现有方法的局限性（如前几节所述），我们在句子的依赖树上利用了图卷积网络。<strong>具体而言，在构造给定句子的依存关系树3之后，我们首先根据句子中的单词获得邻接矩阵A∈Rn×n。重要的是要注意依赖树是有向图</strong>。尽管GCN通常不考虑方向，但可以将其调整为适合方向感知的情况。因此，<strong>我们提出了ASGCN的两个变体，即，在无向的依赖图上的ASGCN-DG，以及在有向的依赖树上的ASGCN-DT。</strong>实际上，ASGCN-DG和ASGCN-DT之间的唯一区别在于它们的邻接矩阵：ASGCN-DT的邻接矩阵比ASGCN-DG的稀疏得多。这样的设置是根据父节点广泛受其子节点影响的现象。此外，遵循Kipf和Welling（2017）中的自循环思想，每个单词都与其相邻位置手动设置，即A的对角线值均为1。</p>
<p>&emsp;&emsp;在第3.1节中<strong>的双向LSTM输出的基础上，以多层方式执行ASGCN变体，即H0 = Hc以使节点了解上下文（Zhang等人，2018）。然后使用具有规范化因子的图卷积运算更新每个节点的表示</strong>（Kipf和Welling，2017），如下所示：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/5.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="5" class="fancybox"><img alt="5" title="5" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/5.png" class="lazyload"></a></p>
<p>&emsp;&emsp;其中gl−1 j是从前一个GCN层演变而来的第j个令牌的表示形式，而hli是当前GCN层的乘积，并且di =？nj = 1Aij是树中第i个令牌的程度。权重Wl and偏重b是可训练参数。</p>
<p>&emsp;&emsp;值得注意的是，<strong>我们没有立即将其送入连续的GCN层，而是首先进行了位置感知转换</strong>：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/6.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="6" class="fancybox"><img alt="6" title="6" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/6.png" class="lazyload"></a></p>
<p>&emsp;&emsp;其中F（·）是分配位置权重的函数，以增强与方面相近的上下文词的重要性。通过这样做，我们的目标是减少依赖项解析过程中自然产生的噪声和偏差。具体来说，函数F（·）为：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/7.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="7" class="fancybox"><img alt="7" title="7" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/7.png" class="lazyload"></a></p>
<p><strong>L层GCN的最终结果是HL</strong>={HL 1，HL 2，··，HLτ+1，··，HLτ+m，··，HL n−1，HL n}，HL t∈R2dh。</p>
<h3 id="Aspect-specific-Masking"><a href="#Aspect-specific-Masking" class="headerlink" title="Aspect-specific Masking"></a><span id="id322">Aspect-specific Masking</span></h3><p>&emsp;&emsp;在这一层中，<strong>我们屏蔽掉非方面词的隐藏状态向量，并保持方面词状态不变:</strong></p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/8.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="8" class="fancybox"><img alt="8" title="8" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/8.png" class="lazyload"></a></p>
<p><strong>该零屏蔽层的输出是面向方面的特征HL= {0，，，hL τ+1，，，hL τ+m，，，0}。</strong>通过图形卷积，这些特征HL Mask以一种既考虑句法依存关系又考虑了远程多词关系的方式在方面周围感知了上下文。</p>
<h2 id="Aspect-aware-Attention"><a href="#Aspect-aware-Attention" class="headerlink" title="Aspect-aware Attention"></a><span id="id33">Aspect-aware Attention</span></h2><p>&emsp;&emsp;基于面向方面的特征，通过新颖的<strong>基于检索的注意力机制生成了隐藏状态向量Hc的精确表示</strong>。这个想法是从隐藏状态向量中检索与方面单词在语义上相关的重要特征，并因此为每个上下文单词设置基于检索的注意力权重。在我们的实现中，注意力权重计算如下：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/9.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="9" class="fancybox"><img alt="9" title="9" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/9.png" class="lazyload"></a></p>
<p>&emsp;&emsp;在这里，点积用于测量方面组成词与句子中的词之间的语义相关性，以便特定方面的掩盖（即零掩盖）可以如公式8所示生效。因此，预测的最终表示形式为：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/10.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="10" class="fancybox"><img alt="10" title="10" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/10.png" class="lazyload"></a></p>
<h2 id="Sentiment-Classification"><a href="#Sentiment-Classification" class="headerlink" title="Sentiment Classification"></a><span id="id34">Sentiment Classification</span></h2><p>&emsp;&emsp;获得表示r后，将其馈入一个全连接层，然后馈入一个softmax归一化层，以在极性决策空间上产生概率分布p∈Rdp：</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/11.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="11" class="fancybox"><img alt="11" title="11" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/11.png" class="lazyload"></a></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a><span id="id35">Training</span></h2><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><span id="id4">Experiments</span></h1><h2 id="Datasets-and-Experimental-Settings"><a href="#Datasets-and-Experimental-Settings" class="headerlink" title="Datasets and Experimental Settings"></a><span id="id41">Datasets and Experimental Settings</span></h2><p>&emsp;&emsp;五个数据集：一个（TWITTER）中包含推特帖子，而其他四个（LAP14，REST14，REST15，REST16）分别来自SemEval 2014任务4，SemEval 2015任务12和SemEval 2016任务5，由笔记本电脑和餐厅两类数据组成。</p>
<h2 id="Models-for-Comparison"><a href="#Models-for-Comparison" class="headerlink" title="　Models for Comparison"></a>　<span id="id42">Models for Comparison</span></h2><ul>
<li><p>SVM（Kiritchenko等人，2014）是使用常规特征提取方法赢得SemEval 2014任务4的模型。</p>
</li>
<li><p>LSTM（Tang等人，2016a）使用LSTM的最后一个隐藏状态向量来预测情绪极性。</p>
</li>
<li><p>MemNet（Tang等人，2016b）将上下文视为外部存储器，并从多跳架构中受益。</p>
</li>
<li><p>AOA（Huang等，2018）从机器翻译领域借鉴了注意力过度注意的概念。 •IAN（Ma等人，2017）以交互方式建模方面及其上下文之间的关系。</p>
</li>
<li><p>TNet-LF（Li等人，2018）提出了上下文保存转换（CPT），以保存和加强上下文的信息部分。</p>
</li>
</ul>
<p>&emsp;&emsp;为了检查GCN在何种程度上胜过CNN，我们在实验中还涉及了一个名为ASCNN的模型，该模型在ASGCN中将2层GCN替换为2层CNN。</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a><span id="id43">Results</span></h2><p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/12.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="12" class="fancybox"><img alt="12" title="12" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/12.png" class="lazyload"></a></p>
<p>&emsp;&emsp;如表2所示，ASGCN-DG在LAP14和REST15数据集上的性能始终优于所有比较模型，并且在TWITTER和REST16数据集上与基线TNet-LF以及在REST14上与ASCNN相比均达到可比的结果。</p>
<p>&emsp;&emsp;结果证明了ASGCN-DG的有效性以及将语法信息直接集成到注意力机制中的不足，如He等人所述。 （2018）。同时，在TWITTER，LAP14，Rest15和REST16数据集上，ASGCN-DG的性能要优于ASGCN-DT。而且ASGCN-DT的结果低于LAP14上的TNet-LF的结果。一个可能的原因是，来自父节点的信息与来自子节点的信息一样重要，因此将依赖树视为有向图会导致信息丢失。</p>
<p>&emsp;&emsp;此外，除REST14之外，ASGCNDG在所有数据集上的表现均优于ASCNN，这说明ASGCN在捕获远程单词依存关系方面更胜一筹，而ASCNN在某种程度上显示了特定于方面的屏蔽带来的影响。我们怀疑REST14数据集对语法信息不太敏感。此外，TWITTER数据集中的句子语法较少，从而限制了效果。我们推测这可能是ASGCN-DG和ASGCN-DT在TWITTER数据集上获得次优结果的原因。</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a><span id="id44">Ablation Study</span></h2><p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/13.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="13" class="fancybox"><img alt="13" title="13" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/13.png" class="lazyload"></a></p>
<p>&emsp;&emsp;为了进一步检查ASGCN的每个组件带来的收益水平，对ASGCN-DG进行了消融研究。结果显示在表3中。我们还提供了BiLSTM + Attn的结果作为基线，该基线分别使用两个LSTM作为方面和上下文。</p>
<p>&emsp;&emsp;首先，移除位置权重（即ASGCN-DG w / o pos）会导致LAP14，REST15和REST16数据集的性能下降，但TWITTER和REST14数据集的性能会提高。回想一下REST14数据集的主要结果，我们得出的结论是，<strong>如果语法对数据不重要，则位置权重的集成无助于减少用户生成内容的噪声。</strong>而且，在摆脱了特定于方面的mask（即不带mask的ASGCN-DG）之后，该模型无法保持与TNet-LF一样的竞争力。这验证了特定于方面的掩盖的重要性。</p>
<p>&emsp;&emsp;与ASGCN-DG相比，不带GCN的ASGCN-DG（即保留位置权重和特定于方面的遮罩，但不使用GCN图层）在TWITTER数据集上的所有五个数据集上除F1度量外都无能为力。但是，由于方面特定的掩盖机制的优势，在除REST14数据集之外的所有数据集上，不带GCN的ASGCNDG仍比BiL-STM + Attn稍好。</p>
<p>&emsp;&emsp;因此可以得出结论，<strong>因为GCN同时捕获了句法性的单词依赖性和远程单词关系，所以GCN在很大程度上有助于ASGCN。</strong>尽管如此，正如我们在TWITTER和REST14数据集中看到的那样，<strong>GCN在对语法信息不敏感的数据集上并没有达到预期的效果。</strong></p>
<h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a><span id="id45">Case Study</span></h2><p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/14.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="14" class="fancybox"><img alt="14" title="14" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/14.png" class="lazyload"></a></p>
<p>&emsp;&emsp;第一个样本”美味的食物，但是服务真糟糕！”一句话中有两个方面，这可能会妨碍基于注意力的模型将这些方面与其相关的描述性词精确对齐。第二个示例句子”员工应该更友好一点。”使用虚拟词”应该”，给检测隐式语义带来额外的困难。最后一个示例在句子中包含否定词，这很容易导致模型做出错误的预测。</p>
<p>&emsp;&emsp;MemNet在所有三个示例中均失败。<strong>尽管IAN可以针对不同方面使用不同的修饰语，但它无法推断具有特殊样式的句子的情感极性。有了位置权重，ASCNN可以正确预测第二个样本的标签，因为该短语应该与方面人员接近，但对于第三个样本，则具有较长的单词依赖性</strong>。我们的ASGCN-DG正确处理了所有三个样本，<strong>这意味着GCN有效地将语法相关性信息集成到了丰富的语义表示中。特别地，ASGCNDG对第二个和第三个样本做出正确的预测，这两个样本都具有看似偏向的焦点。这表明ASGCN能够捕获远程多字功能。</strong></p>
<h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a><span id="id5">Discussion</span></h1><h2 id="Investigation-on-the-Impact-of-GCN（GCN层影响的调查）"><a href="#Investigation-on-the-Impact-of-GCN（GCN层影响的调查）" class="headerlink" title="Investigation on the Impact of GCN（GCN层影响的调查）"></a><span id="id51">Investigation on the Impact of GCN（GCN层影响的调查）</span></h2><p>&emsp;&emsp;由于ASGCN涉及L层GCN，因此我们研究了层号L对ASGCN-DG最终性能的影响。</p>
<p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/15.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="15" class="fancybox"><img alt="15" title="15" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/15.png" class="lazyload"></a></p>
<h2 id="Investigation-on-the-Effect-of-Multiple-Aspects"><a href="#Investigation-on-the-Effect-of-Multiple-Aspects" class="headerlink" title="Investigation on the Effect of Multiple Aspects"></a><span id="id52">Investigation on the Effect of Multiple Aspects</span></h2><p><a href="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/16.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="16" class="fancybox"><img alt="16" title="16" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/16.png" class="lazyload"></a></p>
<p>&emsp;&emsp;在数据集中，<strong>一个句子中可能存在多个方面的术语。因此，我们打算测量这种现象是否会影响ASGCN的有效性。</strong>我们根据句子中方面词的数量将LAP14和REST14数据集中的训练样本分为不同的组，并计算这些组之间的训练精度差异。值得注意的是，将具有7个以上方面的样本作为异常值删除，因为这些样本的大小对于任何有意义的比较而言都太小了。</p>
<p>&emsp;&emsp;从图4中可以看出，<strong>当句子中的方面数大于3时，准确性会发生波动，</strong>这表明在捕获多方面相关性方面的鲁棒性较低，这表明在将来的工作中需要对多方面相关性进行建模。</p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a><span id="id6">Related Work</span></h1><p>&emsp;&emsp;在单词序列上构建神经网络模型，例如CNN（Kim，2014; Johnson and Zhang，2015），RNN（Tang等，2016a）和递归卷积神经网络（RC-CNNs）在情感分析中取得了可喜的表现。但是，<strong>人们也认识到了利用依赖树来捕获单词的远距离关系的有效机制的重要性，但缺乏这种机制。</strong> Tai等。 （2015年）表明，具有依赖树或选区树的LSTM优于CNN。董等。 （2014年）提出了一种使用依赖树的自适应递归神经网络，与强基线相比，该网络取得了竞争性结果。最近的研究表明，一般的基于依赖的模型很难获得与基于注意力的模型相当的结果，因为依赖树无法正确捕获长期的上下文语义信息。我们的工作通过采用图卷积网络（GCN）克服了这一局限性（Kipf and Welling，2017）。</p>
<p>&emsp;&emsp;GCN最近在人工智能领域引起了越来越多的关注，并已应用于自然语言处理（NLP）。 Marcheggiani and Titov（2017）声称GCN可以被认为是LSTM的补充，并提出了基于GCN的语义角色标记模型。 V ashishth等。 （2018）和Zhang等。 （2018）分别在文档约会和关系分类中使用了依赖树上的图卷积。姚等。 （2018）将GCN引入到利用文档单词和单词-单词关系的文本分类中，并在各种最新方法上获得了改进。<strong>我们的工作通过图卷积深入研究了依赖树的影响，并开发了特定于方面的GCN模型，该模型与LSTM架构和注意力机制集成在一起，可以更有效地基于方面进行情感分类。</strong></p>
<h1 id="Conclusions-and-Future-Work"><a href="#Conclusions-and-Future-Work" class="headerlink" title="Conclusions and Future Work"></a><span id="id7">Conclusions and Future Work</span></h1><p>&emsp;&emsp;我们重新审视了针对特定方面的情感分类的现有模型所面临的挑战，并<strong>指出了图卷积网络（GCN）应对这些挑战的适用性。因此，我们提出了一种新颖的网络来采用GCN进行基于方面的情感分类。实验结果表明，GCN通过利用句法信息和远程单词依存关系为整体性能带来好处。</strong></p>
<p>&emsp;&emsp;该研究可能会在以下方面得到进一步改进。首先，这项工作没有利用句法依赖树的边缘信息，即每个边缘的标签。我们计划设计一种考虑边缘标签的特定图神经网络。其次，可以合并领域知识。最后但并非最不重要的是，可以通过捕获方面词之间的依赖性来扩展ASGCN模型以同时判断多个方面的情感。</p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习    </a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文    </a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a><a class="post-meta__tags" href="/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/">情感分析    </a><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络    </a></div><div class="post_share"></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/08/02/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/zhengguanyu/PhotoRepos@v1.1/img/封面.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>分类任务评价指标</span></div></a></div><div class="next-post pull_right"><a href="/2020/08/02/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"><img class="next_cover lazyload" data-src="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>传统的机器学习方法——决策树（上）</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/07/31/论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解/" title="论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.3/img/周郴莲/7.31/图片1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-31</div><div class="relatedPosts_title">论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/01/【论文阅读笔记PCNN】Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks/" title="【论文阅读笔记PCNN】Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200801134611553.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-01</div><div class="relatedPosts_title">【论文阅读笔记PCNN】Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/传统的机器学习方法——决策树（上）/" title="传统的机器学习方法——决策树（上）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">传统的机器学习方法——决策树（上）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/命名实体识别简介（一）/" title="命名实体识别简介（一）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802093111103.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hhcHB5ZXZlcnl5ZGF5,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">命名实体识别简介（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/知识图谱/" title="知识图谱"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.4/img/张明磊/8.2/图片2.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">知识图谱</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/09/传统的机器学习方法——决策树（下）/" title="传统的机器学习方法——决策树（下）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-09</div><div class="relatedPosts_title">传统的机器学习方法——决策树（下）</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 东北石油大学智能技术与自然语言处理实验室</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我们团队的博客！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>