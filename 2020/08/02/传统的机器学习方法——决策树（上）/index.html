<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>传统的机器学习方法——决策树（上） | 东北石油大学智能技术与自然语言处理实验室</title><meta name="description" content="传统的机器学习方法——决策树（上）"><meta name="keywords" content="自然语言处理,深度学习,算法"><meta name="author" content="东北石油大学智能技术与自然语言处理实验室"><meta name="copyright" content="东北石油大学智能技术与自然语言处理实验室"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/ava.jpg"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="传统的机器学习方法——决策树（上）"><meta name="twitter:description" content="传统的机器学习方法——决策树（上）"><meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><meta property="og:type" content="article"><meta property="og:title" content="传统的机器学习方法——决策树（上）"><meta property="og:url" content="https://nlplab406.github.io/2020/08/02/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"><meta property="og:site_name" content="东北石油大学智能技术与自然语言处理实验室"><meta property="og:description" content="传统的机器学习方法——决策树（上）"><meta property="og:image" content="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://nlplab406.github.io/2020/08/02/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"><link rel="prev" title="论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks" href="https://nlplab406.github.io/2020/08/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAspect-based%20Sentiment%20Classification%20with%20Aspect-specific%20Graph%20Convolutional%20Networks/"><link rel="next" title="知识图谱" href="https://nlplab406.github.io/2020/08/02/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">东北石油大学智能技术与自然语言处理实验室</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/ava.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">22</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">30</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#决策树模型介绍"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">决策树模型介绍</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#决策树模型概述"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">决策树模型概述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#决策树与if-then规则"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">决策树与if-then规则</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#决策树主要优点"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">决策树主要优点</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#主要步骤"><span class="toc_mobile_items-number">1.4.</span> <span class="toc_mobile_items-text">主要步骤</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#特征选择"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">特征选择</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#特征选择问题"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">特征选择问题</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#举个例子"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text">举个例子</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#重要定义"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">重要定义</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#熵"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">熵</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#条件熵"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text">条件熵</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#信息增益"><span class="toc_mobile_items-number">2.2.3.</span> <span class="toc_mobile_items-text">信息增益</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#信息增益的算法"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">信息增益的算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法过程"><span class="toc_mobile_items-number">2.3.1.</span> <span class="toc_mobile_items-text">算法过程</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#举个例子-1"><span class="toc_mobile_items-number">2.3.2.</span> <span class="toc_mobile_items-text">举个例子</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#决策树的生成"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">决策树的生成</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#介绍"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#ID3算法"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">ID3算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ID3算法核心"><span class="toc_mobile_items-number">3.2.1.</span> <span class="toc_mobile_items-text">ID3算法核心</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法过程-1"><span class="toc_mobile_items-number">3.2.2.</span> <span class="toc_mobile_items-text">算法过程</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#举个例子-2"><span class="toc_mobile_items-number">3.2.3.</span> <span class="toc_mobile_items-text">举个例子</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ID3算法的不足"><span class="toc_mobile_items-number">3.2.4.</span> <span class="toc_mobile_items-text">ID3算法的不足</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#C4-5算法"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">C4.5算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#C4-5算法的不足"><span class="toc_mobile_items-number">3.3.1.</span> <span class="toc_mobile_items-text">C4.5算法的不足</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#总结"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">总结</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树模型介绍"><span class="toc-number">1.</span> <span class="toc-text">决策树模型介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树模型概述"><span class="toc-number">1.1.</span> <span class="toc-text">决策树模型概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树与if-then规则"><span class="toc-number">1.2.</span> <span class="toc-text">决策树与if-then规则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树主要优点"><span class="toc-number">1.3.</span> <span class="toc-text">决策树主要优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#主要步骤"><span class="toc-number">1.4.</span> <span class="toc-text">主要步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#特征选择"><span class="toc-number">2.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#特征选择问题"><span class="toc-number">2.1.</span> <span class="toc-text">特征选择问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#举个例子"><span class="toc-number">2.1.1.</span> <span class="toc-text">举个例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#重要定义"><span class="toc-number">2.2.</span> <span class="toc-text">重要定义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#熵"><span class="toc-number">2.2.1.</span> <span class="toc-text">熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#条件熵"><span class="toc-number">2.2.2.</span> <span class="toc-text">条件熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#信息增益"><span class="toc-number">2.2.3.</span> <span class="toc-text">信息增益</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息增益的算法"><span class="toc-number">2.3.</span> <span class="toc-text">信息增益的算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法过程"><span class="toc-number">2.3.1.</span> <span class="toc-text">算法过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#举个例子-1"><span class="toc-number">2.3.2.</span> <span class="toc-text">举个例子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树的生成"><span class="toc-number">3.</span> <span class="toc-text">决策树的生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#介绍"><span class="toc-number">3.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3算法"><span class="toc-number">3.2.</span> <span class="toc-text">ID3算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3算法核心"><span class="toc-number">3.2.1.</span> <span class="toc-text">ID3算法核心</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法过程-1"><span class="toc-number">3.2.2.</span> <span class="toc-text">算法过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#举个例子-2"><span class="toc-number">3.2.3.</span> <span class="toc-text">举个例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3算法的不足"><span class="toc-number">3.2.4.</span> <span class="toc-text">ID3算法的不足</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C4-5算法"><span class="toc-number">3.3.</span> <span class="toc-text">C4.5算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#C4-5算法的不足"><span class="toc-number">3.3.1.</span> <span class="toc-text">C4.5算法的不足</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70)"><div id="post-info"><div id="post-title"><div class="posttitle">传统的机器学习方法——决策树（上）</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-08-02<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-08-02</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%92%8C%E5%A9%B7%E5%A9%B7/">和婷婷</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%92%8C%E5%A9%B7%E5%A9%B7/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 6 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="决策树模型介绍"><a href="#决策树模型介绍" class="headerlink" title="决策树模型介绍"></a>决策树模型介绍</h1><h2 id="决策树模型概述"><a href="#决策树模型概述" class="headerlink" title="决策树模型概述"></a>决策树模型概述</h2><p>&emsp;&emsp;分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。<br>&emsp;&emsp;分类的时候，从根节点开始，当前节点设为根节点，当前节点必定是一种特征，根据实例的该特征的取值，向下移动，直到到达叶节点，将实例分到叶节点对应的类中。</p>
<p><a href="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="决策树模型" class="fancybox"><img alt="决策树模型" data-src="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="决策树模型"></a></p>
<h2 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h2><p>&emsp;&emsp;可以将决策树看成一个if-then规则的集合：由决策树的根节点到叶节点的每一条路径构建一条规则；路径上的内部结点的特征对应着if条件，叶节点对应着then结论。决策树的每一条路径都具有一个重要的性质：互斥且完备。这就是说，任何一个实例都被且仅被一条路径或规则覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。<br>&emsp;&emsp;<em>举个例子：if(明天是晴天）then(我将出去玩)</em></p>
<h2 id="决策树主要优点"><a href="#决策树主要优点" class="headerlink" title="决策树主要优点"></a>决策树主要优点</h2><ol>
<li>分类速度快；</li>
<li>具有可读性；</li>
<li>学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新数据集，利用决策树模型进行分类。</li>
</ol>
<h2 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h2><ol>
<li>特征选择问题</li>
<li>决策树的生成</li>
<li>决策树的剪枝</li>
</ol>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><h2 id="特征选择问题"><a href="#特征选择问题" class="headerlink" title="特征选择问题"></a>特征选择问题</h2><p>&emsp;&emsp;特征选择在于选取对训练集具有分类能力的特征。如果利用一个特征进行分类的结果与随即分类的结果没有很大差别，则称这个特征没有分类能力。通常特征选则的准则是信息增益或信息增益比。<br>特征选择是决定用哪个特征来划分特征空间。</p>
<h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a><em>举个例子</em></h3><p>&emsp;&emsp;下表（表5.1）是由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性），具体信息如表所示：<br><a href="https://img-blog.csdnimg.cn/20200802174557798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="贷款申请样本数据集" class="fancybox"><img alt="贷款申请样本数据集" data-src="https://img-blog.csdnimg.cn/20200802174557798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="贷款申请样本数据集"></a><br>&emsp;&emsp;通过上表所给的训练数据构建一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请是，根据申请人的特征利用决策树决定是否批准贷款申请。<br><strong>在说明这个例子之前，必须先了解几个定义。</strong></p>
<h2 id="重要定义"><a href="#重要定义" class="headerlink" title="重要定义"></a>重要定义</h2><h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p><a href="https://img-blog.csdnimg.cn/20200802180036468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="熵" class="fancybox"><img alt="熵" data-src="https://img-blog.csdnimg.cn/20200802180036468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="熵"></a><br>&emsp;&emsp;由熵的定义可知，熵只依赖与X的分布，而与X的取值无关，所以也可以将X的熵记作H（<em>p</em>）,即<br><a href="https://img-blog.csdnimg.cn/2020080218034198.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="熵" class="fancybox"><img alt="熵" data-src="https://img-blog.csdnimg.cn/2020080218034198.png" class="lazyload" title="熵"></a></p>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p><a href="https://img-blog.csdnimg.cn/20200802180611704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="条件熵" class="fancybox"><img alt="条件熵" data-src="https://img-blog.csdnimg.cn/20200802180611704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="条件熵"></a></p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p><a href="https://img-blog.csdnimg.cn/20200802180734432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="信息增益" class="fancybox"><img alt="信息增益" data-src="https://img-blog.csdnimg.cn/20200802180734432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="信息增益"></a></p>
<p>&emsp;&emsp;一般地，熵H(Y)与条件熵H(Y|X)之差成为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
<h2 id="信息增益的算法"><a href="#信息增益的算法" class="headerlink" title="信息增益的算法"></a>信息增益的算法</h2><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p><a href="https://img-blog.csdnimg.cn/20200802181234549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="信息增益的算法过程" class="fancybox"><img alt="信息增益的算法过程" data-src="https://img-blog.csdnimg.cn/20200802181234549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="信息增益的算法过程"></a></p>
<h3 id="举个例子-1"><a href="#举个例子-1" class="headerlink" title="举个例子"></a><em>举个例子</em></h3><p><a href="https://img-blog.csdnimg.cn/20200802181403214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="计算过程" class="fancybox"><img alt="计算过程" data-src="https://img-blog.csdnimg.cn/20200802181403214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="计算过程"></a><a href="https://img-blog.csdnimg.cn/20200802181442245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="例子" class="fancybox"><img alt="例子" data-src="https://img-blog.csdnimg.cn/20200802181442245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="例子"></a></p>
<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp;&emsp;根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止，决策树停止生长。这个过程实际上就是使用满足划分准则的特征不断的将数据集划分成纯度更高，不确定性更小的子集的过程。对于当前数据集的每一次划分，都希望根据某个特征划分之后的各个子集的纯度更高，不确定性更小。<br>&emsp;&emsp;决策树生成算法不唯一，这里仅介绍两种算法——ID3算法和C4.5算法。</p>
<h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><h3 id="ID3算法核心"><a href="#ID3算法核心" class="headerlink" title="ID3算法核心"></a>ID3算法核心</h3><p>&emsp;&emsp;核心：是在决策树各个节点上应用<strong><strong>信息增益准则</strong></strong>选择特征递归地构建决策树。</p>
<h3 id="算法过程-1"><a href="#算法过程-1" class="headerlink" title="算法过程"></a>算法过程</h3><p>&emsp;&emsp;算法的过程为：</p>
<ol>
<li>初始化信息增益的阈值ϵ</li>
<li>判断样本是否为同一类Di ，如果是则返回单节点树T。标记类别为Di。</li>
<li>判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。</li>
<li>计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征Ag。</li>
<li>如果Ag的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。</li>
<li>否则，按特征Ag的不同取值Agi将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为Agi。返回增加了节点的数T。</li>
<li>对于所有的子节点，令D=Di,A=A−{Ag} ，递归调用2-6步，得到子树Ti并返回。</li>
</ol>
<h3 id="举个例子-2"><a href="#举个例子-2" class="headerlink" title="举个例子"></a><em>举个例子</em></h3><p>&emsp;&emsp;用ID3算法构建表5.1的决策树过程如下：</p>
<p><a href="https://img-blog.csdnimg.cn/20200802182533696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="例子" class="fancybox"><img alt="例子" data-src="https://img-blog.csdnimg.cn/20200802182533696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="例子"></a></p>
<p>&emsp;&emsp;构建决策树结果：</p>
<p><a href="https://img-blog.csdnimg.cn/2020080218271422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption="决策树" class="fancybox"><img alt="决策树" data-src="https://img-blog.csdnimg.cn/2020080218271422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70" class="lazyload" title="决策树"></a></p>
<h3 id="ID3算法的不足"><a href="#ID3算法的不足" class="headerlink" title="ID3算法的不足"></a>ID3算法的不足</h3><p>&emsp;&emsp;ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。　　</p>
<ol>
<li>ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。</li>
<li>ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。</li>
<li>ID3算法对于缺失值的情况没有做考虑。</li>
<li>没有考虑过拟合的问题。</li>
</ol>
<h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>&emsp;&emsp;C4.5算法与ID3算法很相似，C4.5算法仅仅是对ID3算法做了改进，在生成决策树过程中采用<strong>信息增益比</strong>来选择特征算法过程没有变化，这里不再赘述。</p>
<h3 id="C4-5算法的不足"><a href="#C4-5算法的不足" class="headerlink" title="C4.5算法的不足"></a>C4.5算法的不足</h3><p>&emsp;&emsp;C4.5虽然改进了ID3算法的几个主要的问题，仍然有优化的空间。</p>
<ol>
<li>由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。</li>
<li>C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</li>
<li>C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</li>
</ol>
<p>&emsp;&emsp;这4个问题在CART树里面部分加以了改进。所以目前如果不考虑集成学习话，在普通的决策树算法里，CART算法算是比较优的算法了。
　   </p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;这篇文章仅简单介绍了决策树的基本内容，主要是决策树的特征选择和生成两部分，后续会继续介绍决策树的剪枝和CART这个比较方便的算法。</p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习    </a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a></div><div class="post_share"></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/08/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAspect-based%20Sentiment%20Classification%20with%20Aspect-specific%20Graph%20Convolutional%20Networks/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.5/img/朱鑫海/8.2/1.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>论文阅读：Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</span></div></a></div><div class="next-post pull_right"><a href="/2020/08/02/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.4/img/张明磊/8.2/图片2.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>知识图谱</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/08/09/传统的机器学习方法——决策树（下）/" title="传统的机器学习方法——决策树（下）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200809164842535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-09</div><div class="relatedPosts_title">传统的机器学习方法——决策树（下）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/命名实体识别简介（一）/" title="命名实体识别简介（一）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802093111103.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hhcHB5ZXZlcnl5ZGF5,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">命名实体识别简介（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/知识图谱/" title="知识图谱"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.4/img/张明磊/8.2/图片2.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">知识图谱</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/10/知识图谱总体构建思路（非结构化文本数据）/" title="知识图谱总体构建思路（非结构化文本数据）"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.8/img/张明磊/8.10/1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-10</div><div class="relatedPosts_title">知识图谱总体构建思路（非结构化文本数据）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/31/论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解/" title="论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.3/img/周郴莲/7.31/图片1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-31</div><div class="relatedPosts_title">论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/15/损失函数/" title="损失函数"><img class="relatedPosts_cover lazyload"data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-15</div><div class="relatedPosts_title">损失函数</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 东北石油大学智能技术与自然语言处理实验室</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我们团队的博客！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>