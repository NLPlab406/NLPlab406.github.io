<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>损失函数 | 东北石油大学智能技术与自然语言处理实验室</title><meta name="description" content="损失函数"><meta name="keywords" content="机器学习,自然语言处理,算法"><meta name="author" content="东北石油大学智能技术与自然语言处理实验室"><meta name="copyright" content="东北石油大学智能技术与自然语言处理实验室"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/ava.jpg"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="损失函数"><meta name="twitter:description" content="损失函数"><meta name="twitter:image" content="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg"><meta property="og:type" content="article"><meta property="og:title" content="损失函数"><meta property="og:url" content="https://nlplab406.github.io/2020/08/15/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"><meta property="og:site_name" content="东北石油大学智能技术与自然语言处理实验室"><meta property="og:description" content="损失函数"><meta property="og:image" content="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://nlplab406.github.io/2020/08/15/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"><link rel="next" title="SSH简便远程登录+通过SSH从服务器下载文件" href="https://nlplab406.github.io/2020/08/15/SSH%E7%AE%80%E4%BE%BF%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95+%E9%80%9A%E8%BF%87SSH%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">东北石油大学智能技术与自然语言处理实验室</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/ava.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">20</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">22</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">29</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#机器学习笔记—损失函数"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">机器学习笔记—损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#为什么使用损失函数？"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">为什么使用损失函数？</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#梯度下降法"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">梯度下降法</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#损失函数的性质"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">损失函数的性质</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#常用的损失函数"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">常用的损失函数</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#均方误差-MSE"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">均方误差(MSE)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#平均绝对误差"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">平均绝对误差</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#MAE与MSE的对比"><span class="toc_mobile_items-number">4.2.1.</span> <span class="toc_mobile_items-text">MAE与MSE的对比</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#交叉熵损失函数"><span class="toc_mobile_items-number">4.3.</span> <span class="toc_mobile_items-text">交叉熵损失函数</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#信息量"><span class="toc_mobile_items-number">4.3.1.</span> <span class="toc_mobile_items-text">信息量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#熵"><span class="toc_mobile_items-number">4.3.2.</span> <span class="toc_mobile_items-text">熵</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#交叉熵"><span class="toc_mobile_items-number">4.3.3.</span> <span class="toc_mobile_items-text">交叉熵</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#自定义损失函数"><span class="toc_mobile_items-number">4.4.</span> <span class="toc_mobile_items-text">自定义损失函数</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#总结"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">总结</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#机器学习笔记—损失函数"><span class="toc-number">1.</span> <span class="toc-text">机器学习笔记—损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么使用损失函数？"><span class="toc-number">2.</span> <span class="toc-text">为什么使用损失函数？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降法"><span class="toc-number">2.1.</span> <span class="toc-text">梯度下降法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数的性质"><span class="toc-number">3.</span> <span class="toc-text">损失函数的性质</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常用的损失函数"><span class="toc-number">4.</span> <span class="toc-text">常用的损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#均方误差-MSE"><span class="toc-number">4.1.</span> <span class="toc-text">均方误差(MSE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#平均绝对误差"><span class="toc-number">4.2.</span> <span class="toc-text">平均绝对误差</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MAE与MSE的对比"><span class="toc-number">4.2.1.</span> <span class="toc-text">MAE与MSE的对比</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#交叉熵损失函数"><span class="toc-number">4.3.</span> <span class="toc-text">交叉熵损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#信息量"><span class="toc-number">4.3.1.</span> <span class="toc-text">信息量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#熵"><span class="toc-number">4.3.2.</span> <span class="toc-text">熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#交叉熵"><span class="toc-number">4.3.3.</span> <span class="toc-text">交叉熵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义损失函数"><span class="toc-number">4.4.</span> <span class="toc-text">自定义损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">损失函数</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-08-15<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-08-15</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%A9%B9%E5%AD%90%E4%BE%9D/">詹子依</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%A9%B9%E5%AD%90%E4%BE%9D/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.9k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 12 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="机器学习笔记—损失函数"><a href="#机器学习笔记—损失函数" class="headerlink" title="机器学习笔记—损失函数"></a>机器学习笔记—损失函数</h2><p>&emsp;&emsp;<em>在学习机器学习（2）的模型的评估与选择的时候，我遇到了一些问题，比如一些关于损失函数的知识，于是我想先把损失函数的内容了解清楚，以便后面的学习。我看了一些博客，以及看了一些相关的视频，总结了这些笔记。如果还有不足的地方，请大家多多指教，我一定好好改正！谢谢大家！</em></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数.jpg" class="lazyload"></a></p>
<p>&emsp;&emsp;长话简说，之前在机器学习（1）提到过，这是一个机器学习的流程。</p>
<p>&emsp;&emsp;比如让这个机器认识猫。这个输入就是猫的一些照片也就是数据，模型就是我们用各种机器学习的算法建立出的模型，然后进行一个预测。刚开始，这个机器还没有学到什么的时候，我们先对它进行一个预测，那么这个时候机器就会猜的很不准，预测值与真实值就会有一个很大的差异，我们把它叫做<u>损失函数</u>。</p>
<p>&emsp;&emsp;而我们的目的就是想让预测值与真实值的差异变小，让他们两个越来越相近。于是用到优化的方法，比如我们常见的<u>梯度下降的方法</u>来最小化这个损失函数，来达到一个<u>更新我们这个模型参数的目的</u>，这样不断地更新，我们就可能下一次的预测值就和我们的真实值就非常相近了。比如这里的猫和狗，我们的机器就能把他们正确的识别出来。</p>
<p>&emsp;&emsp;总之，这个过程就是机器学习的一个过程，这个损失函数就是用来<u>评价模型的一个指标</u>。这个损失函数越大，就说明预测的越不准。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数2.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数2.jpg" class="lazyload"></a></p>
<p>&emsp;&emsp;我们可能这样想过，我们不断训练这个模型，需要达到的目的，就是使我们的正确率越来越高。那为什么我们不以正确率作为指标，用优化参数的方法提升正确率，而是用损失函数来作为评价模型的一个指标呢？</p>
<h2 id="为什么使用损失函数？"><a href="#为什么使用损失函数？" class="headerlink" title="为什么使用损失函数？"></a>为什么使用损失函数？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>个数</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>正确率</td>
<td>0%</td>
<td>10%</td>
<td>20%</td>
<td>30%</td>
<td>40%</td>
<td>50%</td>
<td>60%</td>
<td>70%</td>
<td>80%</td>
<td>90%</td>
<td>100%</td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/正确率.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="0" class="fancybox"><img alt="0" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/正确率.jpg" class="lazyload" title="0"></a></p>
<p>&emsp;&emsp;如果我们用正确率作为指标：假如有十个样本，猜对0个样本的正确率是0%，猜对1个样本的正确率为10%等等。</p>
<p>&emsp;&emsp;那么这个时候权重参数与指标的关系大概就是如图阶梯的形式。比如猜对4个的时候，正确率是40%，于是我们要去调这些参数，来让我们的模型正确率越来越高，但此时可能遇到这样的问题，我们把参数调高一点，正确率是40%，调低一点，正确率也是40%。此时我们就不清楚我们的参数应该调大还是小。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>&emsp;&emsp;我们再来看一下刚刚提到的优化的方法，也就是梯度下降法。我们来简单介绍一下：</p>
<p>1：梯度下降法的场景虚设</p>
<blockquote>
<p>引用《作者：六尺帐篷》</p>
<p>&emsp;&emsp;梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来( 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用<u>梯度下降算法</u>来帮助自己下山。具体来说就是，<strong>以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走</strong>，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。<strong>然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷</strong>。</p>
</blockquote>
<p>&emsp;&emsp;梯度下降的基本过程就和下山的场景类似。</p>
<blockquote>
<p>引用《作者：且行且安》</p>
<p>&emsp;&emsp;首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是<u>找到这个函数的最小值，也就是山底。</u>根据之前的场景假设，<u>最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，</u>对应到函数中，就是<strong>找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！</strong>因为梯度的方向就是函数之变化最快的方向，所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。</p>
</blockquote>
<p>（梯度是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大值，即梯度的方向是函数增长最快的方向，梯度的反方向是函数降低最快的反方向）</p>
<p>2：梯度下降算法的数学解释</p>
<blockquote>
<p>&emsp;&emsp;梯度下降法，是一种寻找函数极小值的方法。该方法最普遍的做法是：在已知参数当前值的情况下，按照当前点对应的梯度向量的反方向，并按照事先给定好的步长大小，对参数进行调整，函数就会逼近一个极小值。</p>
</blockquote>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="1" class="fancybox"><img alt="1" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降.jpg" class="lazyload" title="1"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降法1.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降法1.jpg" class="lazyload" title="2"></a></p>
<p>&emsp;&emsp;如图公式，我们更新后的<u>权重参数=老权重参数-学习率*权重对应的梯度</u></p>
<p>&emsp;&emsp;理解：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是学习率，走完这个段步长，就到达了Θ1这个点</p>
<p>（学习率：通过学习率来控制每一步走的距离，不要走太快，步长太长会错过了最低点。同时也要保证不要走得太慢，太小的话半天都无法收敛）</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降法2.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="3" class="fancybox"><img alt="3" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降法2.jpg" class="lazyload" title="3"></a></p>
<p>&emsp;&emsp;学习率一般是0-1之间，参数每一次更新，都会朝着损失函数最小的方向。比如这个蓝线，代表的是损失函数的图形。</p>
<p>&emsp;&emsp;也就是说这个参数不管初始是在左边还是右边，每一次更新，都会朝损失函数最小的方向去走。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降法3.0.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="4" class="fancybox"><img alt="4" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/梯度下降法3.0.jpg" class="lazyload" title="4"></a></p>
<p>&emsp;&emsp;这也解释了为什么用正确率作为指标不可行，可以回到前面权重参数与指标关系的阶梯图中，可以看到，它的梯度是等于0的。那么根据梯度下降法的这个公式，老权重参数-0=老权重参数，相当于没变。</p>
<p>&emsp;&emsp;因此，我们需要用损失函数来作为指标。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数——梯度下降法.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="5" class="fancybox"><img alt="5" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/损失函数——梯度下降法.jpg" class="lazyload" title="5"></a></p>
<p>&emsp;&emsp;也就是说我们就需要构建出一个与权重参数关系是连续的指标。最好如图中的形式，这样它的最优解很容易找到。</p>
<h2 id="损失函数的性质"><a href="#损失函数的性质" class="headerlink" title="损失函数的性质"></a>损失函数的性质</h2><p>（1）连续可导，且导数不是处处为零</p>
<p>（2）非负；当损失函数最小的时候就是预测值与真实值完全一致的时候，这个时候让损失函数为0，那么它就是一个非负的。</p>
<h2 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h2><p>&emsp;&emsp;首先在<u>回归问题</u>里，我们会用到一些损失函数。先来简单了解一下什么是线性回归。</p>
<p>&emsp;&emsp;在现实生活中常常可碰到这样的情况：变量X和变量y之间有一定关系，但又没密切到可以通过X唯一决定y的程度，我们把这类变量之间的关系称为相关关系（非确定性关系）。如：身高与体重的关系，青少年的身高与年龄的关系等等。</p>
<p>体重≈身高-105</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">体重/kg</th>
<th style="text-align:center">身高/cm</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">81</td>
<td style="text-align:center">185</td>
</tr>
<tr>
<td style="text-align:center">75</td>
<td style="text-align:center">180</td>
</tr>
<tr>
<td style="text-align:center">71</td>
<td style="text-align:center">175</td>
</tr>
<tr>
<td style="text-align:center">64</td>
<td style="text-align:center">170</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;我们需要大概的把身高和体重的关系求出来，也就是这个式子：y≈wx+b，我们需要把w,b的值找出来，这就是回归，如图中的这些点，我们要找到一条直线很好的拟合出来。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/线性回归了解.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="6" class="fancybox"><img alt="6" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/线性回归了解.jpg" class="lazyload" title="6"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/线性回归了解2.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="7" class="fancybox"><img alt="7" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/线性回归了解2.jpg" class="lazyload" title="7"></a></p>
<p>&emsp;&emsp;如何判断这个拟合程度最好？</p>
<p>&emsp;&emsp;因此我们要建立一个损失函数。我们可以从图中看出，蓝色的点是我们真实的一个分布的情况，红色的是我们的预测值。</p>
<h3 id="均方误差-MSE"><a href="#均方误差-MSE" class="headerlink" title="均方误差(MSE)"></a>均方误差(MSE)</h3><p>&emsp;&emsp;均方误差是最常用的回归损失函数，它是我们的目标变量和预测值的差值平方和</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/均方误差.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="8" class="fancybox"><img alt="8" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/均方误差.jpg" class="lazyload" title="8"></a></p>
<script type="math/tex; mode=display">
\hat{yi}=wixi+bi</script><p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/均方误差2.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="9" class="fancybox"><img alt="9" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/均方误差2.jpg" class="lazyload" title="9"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/均方误差3.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="10" class="fancybox"><img alt="10" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/均方误差3.jpg" class="lazyload" title="10"></a></p>
<p>&emsp;&emsp;当用这个损失函数时，我们用梯度下降的方法来更新它，预测值与真实值相差越大，梯度就越大，梯度越大，机器在刚开始误差很大的时候学习，就学得很快，到了非常相近的时候，学起来就比较慢。</p>
<h3 id="平均绝对误差"><a href="#平均绝对误差" class="headerlink" title="平均绝对误差"></a>平均绝对误差</h3><p>&emsp;&emsp;平均绝对误差（MAE）是另一种用于回归模型的损失函数。MAE是目标变量和预测变量之间绝对差值之和</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/4.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="11" class="fancybox"><img alt="11" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/4.jpg" class="lazyload" title="11"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/0.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="12" class="fancybox"><img alt="12" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/0.jpg" class="lazyload" title="12"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/2.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="13" class="fancybox"><img alt="13" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/2.jpg" class="lazyload" title="13"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/8.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="14" class="fancybox"><img alt="14" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/8.jpg" class="lazyload" title="14"></a></p>
<p>&emsp;&emsp;这个损失函数的梯度是个常数，误差很大的时候，学起来是这个速度，误差很小的时候学起来也是这个速度。</p>
<h4 id="MAE与MSE的对比"><a href="#MAE与MSE的对比" class="headerlink" title="MAE与MSE的对比"></a>MAE与MSE的对比</h4><p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814195915.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="15" class="fancybox"><img alt="15" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814195915.jpg" class="lazyload" title="15"></a></p>
<p>&emsp;&emsp;假设有一个球沿着最小值方向往下滚，用MAE这个损失函数的话，就相当于按着同样的速度往最小值的方向滚。到了非常接近的时候还是这个速度，就有可能弹过去，越过去。而用MSE这个损失函数，刚开始学得非常快，到了预测值与真实值很接近的时候，就会慢慢减速，就会防止这个小球从这里越过去，就是这样的好处。</p>
<p>&emsp;&emsp;所以在回归问题里面，一般用均方误差这个损失函数。</p>
<p>&emsp;&emsp;但是有一种情况会用到MAE这种损失函数，比如如下的数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">ID</th>
<th>error</th>
<th>\</th>
<th>error\</th>
<th></th>
<th>error^2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td>-0.1</td>
<td>0.1</td>
<td>0.01</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td>-1.5</td>
<td>1.5</td>
<td>2.25</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td>2</td>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;MAE:0.92</p>
<p>&emsp;&emsp;MSE:1.452</p>
<p>&emsp;&emsp;但是此时，出现一个异常值：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>error</th>
<th>\</th>
<th>error\</th>
<th></th>
<th>error^2</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>15</td>
<td>15</td>
<td>225</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;MAE:3.52</p>
<p>&emsp;&emsp;MSE:45.652</p>
<p>&emsp;&emsp;我们可以看到MAE变化不是很大，而MSE这个损失函数变化很大。</p>
<p>&emsp;&emsp;就比如说：</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814202900.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="16" class="fancybox"><img alt="16" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814202900.jpg" class="lazyload" title="16"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814202906.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="17" class="fancybox"><img alt="17" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814202906.jpg" class="lazyload" title="17"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/00.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="18" class="fancybox"><img alt="18" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/00.jpg" class="lazyload" title="18"></a></p>
<p>&emsp;&emsp;所以在有异常值的情况下，用MAE这个损失函数比用MSE要好一些。</p>
<p>&emsp;&emsp;以上是对于回归的问题，那么在分类的问题里，又是什么情况呢？</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814205936.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="19" class="fancybox"><img alt="19" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814205936.jpg" class="lazyload" title="19"></a></p>
<p>&emsp;&emsp;比如这张图片，我们来区分它是猫还是狗还是老虎，就是预测它大概的概率。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814204157.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="20" class="fancybox"><img alt="20" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814204157.jpg" class="lazyload" title="20"></a></p>
<p>&emsp;&emsp;这里预测值与真实值我们也要建立一个损失函数。在分类里面，我们要用的损失函数是交叉熵。而用MSE也是可以的：</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/999.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="21" class="fancybox"><img alt="21" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/999.jpg" class="lazyload" title="21"></a></p>
<p>&emsp;&emsp;那么为什么我们在分类的时候，不用这个损失函数呢？以逻辑回归为例（二分类问题）</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/888.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="22" class="fancybox"><img alt="22" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/888.jpg" class="lazyload" title="22"></a></p>
<p>&emsp;&emsp;a是预测值，y是真实值，让这个式子最小，就是a=y的时候。我们再来求梯度：</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814225354.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="23" class="fancybox"><img alt="23" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814225354.jpg" class="lazyload" title="23"></a></p>
<p>&emsp;&emsp;a也就是这个sigmoid函数的图像：</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814230216.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="24" class="fancybox"><img alt="24" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200814230216.jpg" class="lazyload" title="24"></a></p>
<p>&emsp;&emsp;我们可以看到在负的方向，它的值是趋于零的，而梯度趋近于零的时候，学习起来是很慢的（如前面：更新后的权重=老权重-学习率*权重对应的梯度，梯度趋近于零，就相当于每一次更新，更新不动了）</p>
<p>&emsp;&emsp;所以在分类问题上，我们不用MSE。</p>
<p>&emsp;&emsp;假如我们的损失函数没有a这一项，直接求得的是（a-y)x,这就比较好了。于是我们可以找到这样一个损失函数，叫做交叉熵损失函数。</p>
<h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><p>&emsp;&emsp;一个事件发生的概率越大，则它携带的信息量就越小</p>
<p>&emsp;&emsp;I(x)=-log(p(x))</p>
<p>&emsp;&emsp;小明平时不爱学习，考试经常不及格，而小红是个勤奋学习的学生，经常得满分，所以我们可以做如下假设：</p>
<p>&emsp;&emsp;事件A：小明考试及格，对应的概率P(xA)=0.1,信息量I(xA)=-log(0.1)=3.3219</p>
<p>&emsp;&emsp;事件B：小红考试及格，对应的概率p(xB)=0.999,信息量I(xB)=-log(0.999)=0.0014</p>
<h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><p>&emsp;&emsp;熵表示不确定度，为信息量的期望</p>
<p>&emsp;&emsp;熵的单位随着公式log运算的底数变化，当底数为2的时候单位为比特，底数为e时，单位为奈特</p>
<p>&emsp;&emsp;H(x)=-[p(x)logp(x)+(1-p(x))log(1-p(x))]</p>
<p>&emsp;&emsp;HA(x)=0.469</p>
<p>&emsp;&emsp;HB(x)=0.114</p>
<p>&emsp;&emsp;当这个人考试的几个的概率是0.5的时候，不确定度最高：</p>
<p>&emsp;&emsp;H(p(x)=0.5)=1</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815011002.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="25" class="fancybox"><img alt="25" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815011002.jpg" class="lazyload" title="25"></a></p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><blockquote>
<p>&emsp;&emsp;p是真实的分布，q是模型预测出来的分布。交叉熵是非对称的，描述的是假设一个预测的概率分布q服从的是真实分布p，所需要的平均信息量。</p>
<p>&emsp;&emsp;如果预测的分布q越来越接近真实的分布p，那么这个信息量就越小。</p>
<p>&emsp;&emsp;我们所需要做的就是优化交叉熵，使其值越小，从而使得模型预测的概率分布越接近真实的概率分布。</p>
</blockquote>
<p>&emsp;&emsp;H(p,q)=-p(x)logq(x)</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815011907.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="26" class="fancybox"><img alt="26" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815011907.jpg" class="lazyload" title="26"></a></p>
<p>&emsp;&emsp;以逻辑回归为例：</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815012237.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="27" class="fancybox"><img alt="27" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815012237.jpg" class="lazyload" title="27"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815013510.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="28" class="fancybox"><img alt="28" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815013510.jpg" class="lazyload" title="28"></a></p>
<p>&emsp;&emsp;于是我们可以看出，我们的预测值与真实值相差越大的时候学得越快，相差越小的时候，就学的越慢。</p>
<p>&emsp;&emsp;这就是在分类问题上，我们使用交叉熵的原因。</p>
<h3 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h3><p>&emsp;&emsp;感知机是一个二分类线性分类模型，输入为实例的特征向量，输出为实例的类别。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815014813.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="29" class="fancybox"><img alt="29" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815014813.jpg" class="lazyload" title="29"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815014918.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="30" class="fancybox"><img alt="30" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815014918.jpg" class="lazyload" title="30"></a></p>
<p>&emsp;&emsp;那么二分类的话，我们怎么来构建它的损失值：</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815015454.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="31" class="fancybox"><img alt="31" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815015454.jpg" class="lazyload" title="31"></a></p>
<p>&emsp;&emsp;比如图中有圆形和三角，我们要做的是找一条直线，将三角形和圆形分开，我们可以看到图中有分类错误的图形，我们将误分类的点的个数来作为损失函数。</p>
<p>&emsp;&emsp;假如误分类的点为5个，我们将它下降到3个，2个，1个的话，可以看到它与用正确率来当损失函数是一样的，是不连续的。</p>
<p>&emsp;&emsp;所以我们要另外想一种方法来构建损失函数。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815020423.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="32" class="fancybox"><img alt="32" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815020423.jpg" class="lazyload" title="32"></a></p>
<p>&emsp;&emsp;我们可以将这些误分类的点到直线的距离的和加起来作为损失函数。</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021709.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="33" class="fancybox"><img alt="33" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021709.jpg" class="lazyload" title="33"></a></p>
<p>（比如向量W=（w1,w2,w3…wn)  ||w||=√(w1^2+w2^2+…+wn^2))</p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021718.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="34" class="fancybox"><img alt="34" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021718.jpg" class="lazyload" title="34"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021713.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="35" class="fancybox"><img alt="35" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021713.jpg" class="lazyload" title="35"></a></p>
<p><a href="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021721.jpg" target="_blank" rel="noopener" data-fancybox="group" data-caption="36" class="fancybox"><img alt="36" data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/微信图片_20200815021721.jpg" class="lazyload" title="36"></a></p>
<p>(其中M为误分类点的集合)</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;回归：mse</p>
<p>&emsp;&emsp;分类：交叉熵</p>
<p>&emsp;&emsp;自定义损失函数</p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习    </a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a></div><div class="post_share"></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/08/15/SSH%E7%AE%80%E4%BE%BF%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95+%E9%80%9A%E8%BF%87SSH%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"><img class="next_cover lazyload" data-src="https://img-blog.csdnimg.cn/20200815094201473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>SSH简便远程登录+通过SSH从服务器下载文件</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/08/08/机器学习（1）/" title="机器学习（1）"><img class="relatedPosts_cover lazyload"data-src="https://gitee.com/zzyaiml/blogimg/raw/master/https://gitee.com/zzyaiml/blogimg/1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-08</div><div class="relatedPosts_title">机器学习（1）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/31/论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解/" title="论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/NLPlab406/NLPlab406.github.io@v1.3/img/周郴莲/7.31/图片1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-07-31</div><div class="relatedPosts_title">论文阅读：What Do We Understand About Convolutional Networks？ 对卷积的了解</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/11/【自我学习】胶囊网络CapsNet/" title="【自我学习】胶囊网络CapsNet"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200402110030173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1YW45NTAyMDU=,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-11</div><div class="relatedPosts_title">【自我学习】胶囊网络CapsNet</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/09/网站识别方案/" title="网站识别方案"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200809154715396.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc3Nzg0MQ==,size_16,color_FFFFFF,t_70#pic_center"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-09</div><div class="relatedPosts_title">网站识别方案</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/13/西瓜书笔记：第3章·线性模型/" title="西瓜书笔记：第3章·线性模型"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200813024012574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwNzcxNA==,size_16,color_FFFFFF,t_70#pic_center"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-13</div><div class="relatedPosts_title">西瓜书笔记：第3章·线性模型</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/02/传统的机器学习方法——决策树（上）/" title="传统的机器学习方法——决策树（上）"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20200802172055879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80OTMxMzMxOQ==,size_16,color_FFFFFF,t_70"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-02</div><div class="relatedPosts_title">传统的机器学习方法——决策树（上）</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 东北石油大学智能技术与自然语言处理实验室</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我们团队的博客！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>